{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vatiza/Machine_Learning/blob/main/Armed_Conflict_Using_Graph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-wOWXDTg_-c",
        "cell_id": "c4e1cf36eeb343ff862a8386814f96e0",
        "deepnote_cell_type": "markdown"
      },
      "source": [
        "# Base Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYB4PwhGgmFZ",
        "source_hash": null,
        "execution_start": 1689137203814,
        "execution_millis": 3045,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "89be28ffdc9f448795d6a8a010061074",
        "deepnote_cell_type": "code",
        "outputId": "9ce73f93-f82c-4fae-8146-f795290102f7"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Datasets/Armed_conflict_2001-01-01-2021-11-01-South_Asia-Bangladesh.csv')\n",
        "\n",
        "remove_column=['data_id','iso','event_id_cnty','event_id_no_cnty','time_precision','notes','timestamp',\n",
        "               'iso3','region','country','geo_precision','source','source_scale','notes','interaction',\n",
        "               'assoc_actor_1','assoc_actor_2','inter1','inter2'\n",
        "               ]\n",
        "df.drop(columns=remove_column,inplace=True)\n",
        "\n",
        "from datetime import datetime\n",
        "month=[]\n",
        "\n",
        "for date_str in df['event_date']:\n",
        "  date_object = datetime.strptime(date_str, '%d %B %Y').date()\n",
        "  month.append(date_object.month)\n",
        "\n",
        "df['month']=month\n",
        "\n",
        "\n",
        "df.drop(columns=['event_date'],inplace=True)\n",
        "my_column = df.pop('month')\n",
        "df.insert(3, 'month', my_column)\n",
        "my_column = df.pop('month')\n",
        "df.insert(1, 'month', my_column)\n",
        "\n",
        "df = df.replace(np.nan, \"unknown\")\n",
        "\n",
        "# filter actor1s having count less than 124 to \"other\"\n",
        "counts = df['actor1'].value_counts()\n",
        "df['actor1'] = df['actor1'].apply(lambda x: 'other' if counts[x] < 193 else x)\n",
        "\n",
        "# filter actor2s having count less than 124 to \"other\"\n",
        "counts = df['actor2'].value_counts()\n",
        "df['actor2'] = df['actor2'].apply(lambda x: 'other' if counts[x] < 193 else x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/Colab Notebooks/Datasets/Armed_conflict_2001-01-01-2021-11-01-South_Asia-Bangladesh.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/content/drive/MyDrive/Colab Notebooks/Datasets/Armed_conflict_2001-01-01-2021-11-01-South_Asia-Bangladesh.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m remove_column\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_id\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miso\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevent_id_cnty\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevent_id_no_cnty\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_precision\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnotes\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      7\u001b[0m                \u001b[38;5;124m'\u001b[39m\u001b[38;5;124miso3\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregion\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcountry\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeo_precision\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource_scale\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnotes\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minteraction\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      8\u001b[0m                \u001b[38;5;124m'\u001b[39m\u001b[38;5;124massoc_actor_1\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124massoc_actor_2\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minter1\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minter2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      9\u001b[0m                ]\n\u001b[1;32m     10\u001b[0m df\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39mremove_column,inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "File \u001b[0;32m/shared-libs/python3.9/py/lib/python3.9/site-packages/pandas/io/parsers.py:610\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    605\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    606\u001b[0m     dialect, delimiter, delim_whitespace, engine, sep, defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m    607\u001b[0m )\n\u001b[1;32m    608\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 610\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/shared-libs/python3.9/py/lib/python3.9/site-packages/pandas/io/parsers.py:462\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    459\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    461\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 462\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
            "File \u001b[0;32m/shared-libs/python3.9/py/lib/python3.9/site-packages/pandas/io/parsers.py:819\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwds:\n\u001b[1;32m    817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 819\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/shared-libs/python3.9/py/lib/python3.9/site-packages/pandas/io/parsers.py:1050\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1046\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1047\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown engine: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mengine\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (valid options are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmapping\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1048\u001b[0m     )\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;66;03m# error: Too many arguments for \"ParserBase\"\u001b[39;00m\n\u001b[0;32m-> 1050\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/shared-libs/python3.9/py/lib/python3.9/site-packages/pandas/io/parsers.py:1867\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1864\u001b[0m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musecols\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39musecols\n\u001b[1;32m   1866\u001b[0m \u001b[38;5;66;03m# open handles\u001b[39;00m\n\u001b[0;32m-> 1867\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open_handles\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1868\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1869\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
            "File \u001b[0;32m/shared-libs/python3.9/py/lib/python3.9/site-packages/pandas/io/parsers.py:1362\u001b[0m, in \u001b[0;36mParserBase._open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_handles\u001b[39m(\u001b[38;5;28mself\u001b[39m, src: FilePathOrBuffer, kwds: Dict[\u001b[38;5;28mstr\u001b[39m, Any]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1359\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1360\u001b[0m \u001b[38;5;124;03m    Let the readers open IOHanldes after they are done with their potential raises.\u001b[39;00m\n\u001b[1;32m   1361\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1362\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1364\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1365\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1366\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1367\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1368\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1369\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/shared-libs/python3.9/py/lib/python3.9/site-packages/pandas/io/common.py:647\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    645\u001b[0m         errors \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    646\u001b[0m     \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 647\u001b[0m     handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m        \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    655\u001b[0m     \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    656\u001b[0m     handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Colab Notebooks/Datasets/Armed_conflict_2001-01-01-2021-11-01-South_Asia-Bangladesh.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwVdL4y6-OUh",
        "source_hash": null,
        "deepnote_to_be_reexecuted": true,
        "cell_id": "8bdc8d5b21c6479e902f0975146ede6b",
        "deepnote_cell_type": "code"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMrsc2uGgSHb",
        "cell_id": "381a387cf22d4236985a921c426aa056",
        "deepnote_cell_type": "markdown"
      },
      "source": [
        "# First Day"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1Xekzy-9I0d",
        "source_hash": null,
        "deepnote_to_be_reexecuted": true,
        "cell_id": "943f44c69e5d4cb4bbd19b8bc53a2989",
        "deepnote_cell_type": "code"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Datasets/Armed_conflict_2001-01-01-2021-11-01-South_Asia-Bangladesh.csv')\n",
        "\n",
        "remove_column=['data_id','iso','event_id_cnty','event_id_no_cnty','time_precision','notes','timestamp','iso3','region','country',\n",
        "               'geo_precision','source','source_scale','notes','interaction', 'assoc_actor_1','assoc_actor_2','inter1','inter2'\n",
        "               ]\n",
        "df.drop(columns=remove_column,inplace=True)\n",
        "\n",
        "from datetime import datetime\n",
        "month=[]\n",
        "\n",
        "for date_str in df['event_date']:\n",
        "  date_object = datetime.strptime(date_str, '%d %B %Y').date()\n",
        "  month.append(date_object.month)\n",
        "\n",
        "df['month']=month\n",
        "\n",
        "\n",
        "df.drop(columns=['event_date'],inplace=True)\n",
        "my_column = df.pop('month')\n",
        "df.insert(3, 'month', my_column)\n",
        "my_column = df.pop('month')\n",
        "df.insert(1, 'month', my_column)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCtcVhso96RT",
        "source_hash": null,
        "deepnote_to_be_reexecuted": true,
        "cell_id": "a48700a5120a4929b4f28a0dd507bf5b",
        "deepnote_cell_type": "code"
      },
      "source": [
        "df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLGLmPStD_UZ",
        "source_hash": null,
        "deepnote_to_be_reexecuted": true,
        "cell_id": "960b3448808448298c6d9801722fe515",
        "deepnote_cell_type": "code"
      },
      "source": [
        "df['event_type'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vghB1UoSEGwX",
        "source_hash": null,
        "deepnote_to_be_reexecuted": true,
        "cell_id": "e57a081a8ba84c01a4f4c757474714b0",
        "deepnote_cell_type": "code"
      },
      "source": [
        "df['actor1'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsNbgHC8E_lp",
        "source_hash": null,
        "deepnote_to_be_reexecuted": true,
        "cell_id": "387f291296214921985b92d756748d6e",
        "deepnote_cell_type": "code"
      },
      "source": [
        "df = df.replace(np.nan, \"unknown\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMeS0bcfEkpN",
        "source_hash": null,
        "deepnote_to_be_reexecuted": true,
        "cell_id": "2bb01cc65cf74093b3a9b712f18e2d45",
        "deepnote_cell_type": "code"
      },
      "source": [
        "# filter actor1s having count less than 124 to \"other\"\n",
        "counts = df['actor1'].value_counts()\n",
        "df['actor1'] = df['actor1'].apply(lambda x: 'other' if counts[x] < 193 else x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5N1isc1kEs1p",
        "source_hash": null,
        "deepnote_to_be_reexecuted": true,
        "cell_id": "894e616c102f492bb466a388908f8a98",
        "deepnote_cell_type": "code"
      },
      "source": [
        "# filter actor2s having count less than 124 to \"other\"\n",
        "counts = df['actor2'].value_counts()\n",
        "df['actor2'] = df['actor2'].apply(lambda x: 'other' if counts[x] < 193 else x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlJPoOwfDrq4",
        "source_hash": null,
        "deepnote_to_be_reexecuted": true,
        "cell_id": "6fac0d0582174a64b5a02019b0b9e89f",
        "deepnote_cell_type": "code"
      },
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create an empty bipartite graph\n",
        "G = nx.Graph()\n",
        "\n",
        "# Extract actors and event types from the dataset\n",
        "actors = set(df['actor1']).union(set(df['actor2']))\n",
        "event_types = set(df['event_type'])\n",
        "\n",
        "# Add nodes for actors and event types\n",
        "G.add_nodes_from(actors, bipartite=0)  # actors are in set 0\n",
        "G.add_nodes_from(event_types, bipartite=1)  # event types are in set 1\n",
        "\n",
        "# Add edges between actors and event types\n",
        "for _, row in df.iterrows():\n",
        "    actor1 = row['actor1']\n",
        "    actor2 = row['actor2']\n",
        "    event_type = row['event_type']\n",
        "    G.add_edge(actor1, event_type)\n",
        "    G.add_edge(actor2, event_type)\n",
        "\n",
        "\n",
        "# Extend the image size\n",
        "plt.figure(figsize=(18, 6))  # Adjust the figsize as per your preference\n",
        "\n",
        "# Visualize the bipartite graph\n",
        "pos = nx.bipartite_layout(G, actors)\n",
        "nx.draw_networkx(G, pos=pos, with_labels=True, font_size=6)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkKaXNdEJhUb",
        "source_hash": null,
        "deepnote_to_be_reexecuted": true,
        "cell_id": "59d57bf82d184f3d9e34ae30f5a958d6",
        "deepnote_cell_type": "code"
      },
      "source": [
        "# area and events are node, actors involved are edges\n",
        "\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create an empty graph\n",
        "G = nx.Graph()\n",
        "\n",
        "# Extract unique administrative areas and event types from the dataset\n",
        "admin_areas = set(df['admin1'])\n",
        "event_types = set(df['event_type'])\n",
        "\n",
        "# Add nodes for administrative areas and event types\n",
        "G.add_nodes_from(admin_areas, bipartite=0)  # administrative areas are in set 0\n",
        "G.add_nodes_from(event_types, bipartite=1)  # event types are in set 1\n",
        "\n",
        "# Add edges between administrative areas and event types based on actor involvement\n",
        "for _, row in df.iterrows():\n",
        "    admin_area = row['admin1']\n",
        "    event_type = row['event_type']\n",
        "    actors_involved = [row['actor1'], row['actor2']]\n",
        "\n",
        "    # Add edges between administrative area and event type for each actor involved\n",
        "    for actor in actors_involved:\n",
        "        G.add_edge(admin_area, event_type, actor=actor)\n",
        "\n",
        "# Extend the image size\n",
        "plt.figure(figsize=(10, 6))  # Adjust the figsize as per your preference\n",
        "\n",
        "# Visualize the graph\n",
        "pos = nx.spring_layout(G, k=0.1)  # Adjust the layout algorithm and parameters as per your preference\n",
        "nx.draw_networkx(G, pos=pos, with_labels=True, font_size=8)  # Adjust font_size as per your preference\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyuUS2G3JwsW",
        "source_hash": null,
        "deepnote_to_be_reexecuted": true,
        "cell_id": "98f34e129a9e4f2b807e085e121e68ef",
        "deepnote_cell_type": "code"
      },
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create an empty graph\n",
        "G = nx.Graph()\n",
        "\n",
        "# Extract unique administrative areas and event types from the dataset\n",
        "admin_areas = set(df['admin1'])\n",
        "event_types = set(df['event_type'])\n",
        "\n",
        "# Add nodes for administrative areas and event types\n",
        "G.add_nodes_from(admin_areas, bipartite=0)  # administrative areas are in set 0\n",
        "G.add_nodes_from(event_types, bipartite=1)  # event types are in set 1\n",
        "\n",
        "# Add edges between administrative areas and event types based on actor involvement\n",
        "for _, row in df.iterrows():\n",
        "    admin_area = row['admin1']\n",
        "    event_type = row['event_type']\n",
        "    actors_involved = [row['actor1'], row['actor2']]\n",
        "\n",
        "    # Add edges between administrative area and event type for each actor involved\n",
        "    for actor in actors_involved:\n",
        "        G.add_edge(admin_area, event_type, actor=actor)\n",
        "\n",
        "# Extend the image size\n",
        "plt.figure(figsize=(16, 10))  # Adjust the figsize as per your preference\n",
        "\n",
        "# Visualize the graph\n",
        "pos = nx.spring_layout(G, k=0.1)  # Adjust the layout algorithm and parameters as per your preference\n",
        "nx.draw_networkx(G, pos=pos, with_labels=True, font_size=8)  # Adjust font_size as per your preference\n",
        "\n",
        "# Add edge labels\n",
        "edge_labels = nx.get_edge_attributes(G, 'actor')\n",
        "nx.draw_networkx_edge_labels(G, pos=pos, edge_labels=edge_labels, font_size=6)\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvfKvyBELX2x",
        "source_hash": null,
        "deepnote_to_be_reexecuted": true,
        "cell_id": "b63535c840a84f1ea95efb46f06f8b28",
        "deepnote_cell_type": "code"
      },
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create an empty graph\n",
        "G = nx.Graph()\n",
        "\n",
        "# Extract unique administrative areas and event types from the dataset\n",
        "admin_areas = set(df['admin1'])\n",
        "event_types = set(df['event_type'])\n",
        "\n",
        "# Add nodes for administrative areas and event types\n",
        "G.add_nodes_from(admin_areas, bipartite=0)  # administrative areas are in set 0\n",
        "G.add_nodes_from(event_types, bipartite=1)  # event types are in set 1\n",
        "\n",
        "# Add edges between administrative areas and event types based on actor involvement\n",
        "for _, row in df.iterrows():\n",
        "    admin_area = row['admin1']\n",
        "    event_type = row['event_type']\n",
        "    actors_involved = [row['actor1'], row['actor2']]\n",
        "\n",
        "    # Add edges between administrative area and event type for each actor involved\n",
        "    for actor in actors_involved:\n",
        "        G.add_edge(admin_area, event_type, actor=actor)\n",
        "\n",
        "# Extend the image size\n",
        "plt.figure(figsize=(18, 10))  # Adjust the figsize as per your preference\n",
        "\n",
        "# Visualize the graph\n",
        "pos = nx.spring_layout(G, k=0.1)  # Adjust the layout algorithm and parameters as per your preference\n",
        "nx.draw_networkx(G, pos=pos, with_labels=True, font_size=8)  # Adjust font_size as per your preference\n",
        "\n",
        "# Add edge labels\n",
        "edge_labels = nx.get_edge_attributes(G, 'actor')\n",
        "nx.draw_networkx_edge_labels(G, pos=pos, edge_labels=edge_labels,font_size=6)\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R77b5PuvMYeJ",
        "source_hash": null,
        "deepnote_to_be_reexecuted": true,
        "cell_id": "992c06edde8f4a42925d194930278511",
        "deepnote_cell_type": "code"
      },
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Filter the data for Dhaka and Rajshahi administrative areas\n",
        "filtered_data = df[(df['admin1'] == 'Dhaka') | (df['admin1'] == 'Rajshahi')]\n",
        "\n",
        "# Create an empty graph\n",
        "G = nx.Graph()\n",
        "\n",
        "# Extract unique administrative areas and event types from the filtered data\n",
        "admin_areas = set(filtered_data['admin1'])\n",
        "event_types = set(filtered_data['event_type'])\n",
        "\n",
        "# Add nodes for administrative areas and event types\n",
        "G.add_nodes_from(admin_areas, bipartite=0)  # administrative areas are in set 0\n",
        "G.add_nodes_from(event_types, bipartite=1)  # event types are in set 1\n",
        "\n",
        "# Add edges between administrative areas and event types based on actor involvement\n",
        "for _, row in filtered_data.iterrows():\n",
        "    admin_area = row['admin1']\n",
        "    event_type = row['event_type']\n",
        "    actors_involved = [row['actor1'], row['actor2']]\n",
        "\n",
        "    # Add edges between administrative area and event type for each actor involved\n",
        "    for actor in actors_involved:\n",
        "        G.add_edge(admin_area, event_type, actor=actor)\n",
        "\n",
        "# Extend the image size\n",
        "plt.figure(figsize=(10, 6))  # Adjust the figsize as per your preference\n",
        "\n",
        "# Visualize the graph\n",
        "pos = nx.spring_layout(G, k=0.1)  # Adjust the layout algorithm and parameters as per your preference\n",
        "nx.draw_networkx(G, pos=pos, with_labels=True, font_size=8)  # Adjust font_size as per your preference\n",
        "\n",
        "# Add edge labels\n",
        "edge_labels = nx.get_edge_attributes(G, 'actor')\n",
        "nx.draw_networkx_edge_labels(G, pos=pos, edge_labels=edge_labels,font_size=6)\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCxZVOdRS_fS",
        "source_hash": null,
        "deepnote_to_be_reexecuted": true,
        "cell_id": "b3720a21e05d4dd2ac6cdaa8197372c6",
        "deepnote_cell_type": "code"
      },
      "source": [
        "!pip install geopandas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjGQyl9SS3ca",
        "source_hash": null,
        "deepnote_to_be_reexecuted": true,
        "cell_id": "08e05914b38346779c7477d6a9166cab",
        "deepnote_cell_type": "code"
      },
      "source": [
        "import geopandas as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a GeoDataFrame from the conflict events dataset\n",
        "gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['longitude'], df['latitude']))\n",
        "\n",
        "# Plot the conflict events on a map\n",
        "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "world.boundary.plot(ax=ax, linewidth=0.5)\n",
        "gdf.plot(ax=ax, markersize=5, color='red')\n",
        "plt.title('Conflict Events')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH-LAUIGTENJ",
        "source_hash": null,
        "deepnote_to_be_reexecuted": true,
        "cell_id": "c47f3d347d3549dd803f14eda117f492",
        "deepnote_cell_type": "code"
      },
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "from scipy.stats import gaussian_kde\n",
        "\n",
        "# Perform spatial clustering using DBSCAN\n",
        "coords = df[['longitude', 'latitude']].values\n",
        "clusterer = DBSCAN(eps=0.1, min_samples=5)\n",
        "labels = clusterer.fit_predict(coords)\n",
        "\n",
        "# Create a GeoDataFrame with cluster labels\n",
        "gdf['cluster'] = labels\n",
        "\n",
        "# Plot clusters on a map\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "world.boundary.plot(ax=ax, linewidth=0.5)\n",
        "gdf.plot(column='cluster', cmap='Set1', markersize=5, legend=True, ax=ax)\n",
        "plt.title('Spatial Clustering')\n",
        "plt.show()\n",
        "\n",
        "# Perform kernel density estimation\n",
        "kde = gaussian_kde(coords.T)\n",
        "x, y = np.mgrid[min(df['longitude']):max(df['longitude']):100j, min(df['latitude']):max(df['latitude']):100j]\n",
        "positions = np.vstack([x.ravel(), y.ravel()])\n",
        "density = np.reshape(kde(positions).T, x.shape)\n",
        "\n",
        "# Plot kernel density estimation\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "world.boundary.plot(ax=ax, linewidth=0.5)\n",
        "ax.imshow(density, cmap='hot', origin='lower', extent=[min(df['longitude']), max(df['longitude']), min(df['latitude']), max(df['latitude'])])\n",
        "plt.title('Kernel Density Estimation')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iH2MyNWNYFFE",
        "source_hash": null,
        "deepnote_to_be_reexecuted": true,
        "cell_id": "9de9e43435e5470f92cdef4b44176748",
        "deepnote_cell_type": "code"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Nu2MsppTab6",
        "source_hash": null,
        "deepnote_to_be_reexecuted": true,
        "cell_id": "fa1dee4571d6428896a64613eabef195",
        "deepnote_cell_type": "code"
      },
      "source": [
        "import networkx as nx\n",
        "\n",
        "# Create a graph\n",
        "G = nx.Graph()\n",
        "\n",
        "# Add nodes for administrative areas\n",
        "admin_areas = set(df['admin1'])\n",
        "G.add_nodes_from(admin_areas)\n",
        "\n",
        "# Add edges between administrative areas based on conflict events\n",
        "for _, row in df.iterrows():\n",
        "    admin1 = row['admin1']\n",
        "    actor1 = row['actor1']\n",
        "    actor2 = row['actor2']\n",
        "    G.add_edge(admin1, actor1)\n",
        "    G.add_edge(admin1, actor2)\n",
        "\n",
        "# Calculate centrality measures\n",
        "degree_centrality = nx.degree_centrality(G)\n",
        "betweenness_centrality = nx.betweenness_centrality(G)\n",
        "\n",
        "# Print centrality measures for each administrative area\n",
        "for admin_area in admin_areas:\n",
        "    print(f\"Administrative Area: {admin_area}\")\n",
        "    print(f\"Degree Centrality: {degree_centrality[admin_area]}\")\n",
        "    print(f\"Betweenness Centrality: {betweenness_centrality[admin_area]}\")\n",
        "    print(\"-----------------------------\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4OlnIRIUHWq",
        "source_hash": null,
        "deepnote_to_be_reexecuted": true,
        "cell_id": "01348b58c7b54291a38be792b558aa7c",
        "deepnote_cell_type": "code"
      },
      "source": [
        "import networkx as nx\n",
        "\n",
        "# Construct the network\n",
        "G = nx.Graph()\n",
        "for _, row in df.iterrows():\n",
        "    admin1 = row['admin1']\n",
        "    event_type = row['event_type']\n",
        "    actors = [row['actor1'], row['actor2']]\n",
        "    G.add_edge(admin1, event_type, actors=actors)\n",
        "\n",
        "# Perform community detection\n",
        "communities = nx.algorithms.community.greedy_modularity_communities(G)\n",
        "\n",
        "# Create a dictionary to store community membership for each administrative area\n",
        "admin_communities = {}\n",
        "\n",
        "# Populate the community membership for each administrative area\n",
        "for i, community in enumerate(communities):\n",
        "    for node in community:\n",
        "        if node.startswith('admin1'):\n",
        "            admin_communities[node] = i\n",
        "\n",
        "# Print community membership for each administrative area\n",
        "for admin_area, community_id in admin_communities.items():\n",
        "    print(f\"Administrative Area: {admin_area}, Community: {community_id}\")\n",
        "    print(\"-----------------------------\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gB9Vei2tgalD",
        "cell_id": "d977a8b0938a48d7ad0ef9358aa41b16",
        "deepnote_cell_type": "markdown"
      },
      "source": [
        "# Day 2 - Goal Find Clusters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-IPKdfoghAt",
        "source_hash": null,
        "deepnote_to_be_reexecuted": true,
        "cell_id": "bf4914498b304e86ad5ff5556b0da79a",
        "deepnote_cell_type": "code"
      },
      "source": [
        "df.sample(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywfK-oFzlGtL",
        "cell_id": "20618f7afef14034aa1e73c75d9dfd83",
        "deepnote_cell_type": "markdown"
      },
      "source": [
        "## girvan_newman"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGaSWPJehO9C",
        "source_hash": null,
        "deepnote_to_be_reexecuted": true,
        "cell_id": "3fd3ccd5aaa247b6b8a9b151033aadc2",
        "deepnote_cell_type": "code"
      },
      "source": [
        "# I want to cluster the actors based on event_type using graph clustering\n",
        "import networkx as nx\n",
        "from networkx.algorithms.community import girvan_newman\n",
        "\n",
        "# Step 1: Preprocess the df and extract relevant columns (event_type, actor1, actor2)\n",
        "\n",
        "# Step 2: Create a graph\n",
        "G = nx.Graph()\n",
        "\n",
        "# Step 3: Add edges based on shared event types between actors\n",
        "for index, row in df.iterrows():\n",
        "    event_type = row['event_type']\n",
        "    actor1 = row['actor1']\n",
        "    actor2 = row['actor2']\n",
        "\n",
        "    # Add edge between actor1 and actor2 with the event_type as the weight\n",
        "    if actor1 != actor2:\n",
        "        G.add_edge(actor1, actor2, weight=event_type)\n",
        "\n",
        "# Step 4: Apply Girvan-Newman algorithm for graph clustering\n",
        "communities = girvan_newman(G)\n",
        "\n",
        "# Step 5: Extract the communities/clusters\n",
        "clusters = tuple(sorted(c) for c in next(communities))\n",
        "\n",
        "# Print the clusters\n",
        "for i, cluster in enumerate(clusters):\n",
        "    print(f\"Cluster {i+1}: {cluster}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z--VjY50lKcx",
        "cell_id": "af22c556948840a6ab403021250bb7d8",
        "deepnote_cell_type": "markdown"
      },
      "source": [
        "## SpectralClustering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGgsvtL0jijt",
        "source_hash": null,
        "deepnote_to_be_reexecuted": true,
        "cell_id": "910caef507514fd68b12389c96c93216",
        "deepnote_cell_type": "code"
      },
      "source": [
        "from sklearn.cluster import SpectralClustering\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import scipy.sparse as sp\n",
        "\n",
        "# Step 1: Preprocess the df and extract relevant columns (event_type, actor1, actor2)\n",
        "\n",
        "# Step 2: Create a graph\n",
        "G = nx.Graph()\n",
        "\n",
        "# Step 3: Add edges based on shared event types between actors\n",
        "for index, row in df.iterrows():\n",
        "    event_type = row['event_type']\n",
        "    actor1 = row['actor1']\n",
        "    actor2 = row['actor2']\n",
        "\n",
        "    # Add edge between actor1 and actor2 with the event_type as the weight\n",
        "    if actor1 != actor2:\n",
        "        G.add_edge(actor1, actor2, weight=event_type)\n",
        "\n",
        "# Step 4: Convert event types to numeric values\n",
        "label_encoder = LabelEncoder()\n",
        "df['event_type_numeric'] = label_encoder.fit_transform(df['event_type'])\n",
        "\n",
        "# Step 5: Create an adjacency matrix with numeric event types\n",
        "adj_matrix = nx.adjacency_matrix(G, weight='event_type_numeric')\n",
        "adj_matrix_sparse = sp.csr_matrix(adj_matrix)\n",
        "\n",
        "# Step 6: Apply Spectral Clustering\n",
        "sc = SpectralClustering(n_clusters=4, affinity='precomputed', assign_labels='discretize')\n",
        "sc.fit(adj_matrix_sparse)\n",
        "\n",
        "# Step 7: Get the cluster labels and actors grouped by cluster\n",
        "cluster_labels = sc.labels_\n",
        "actors_by_cluster = {}\n",
        "for i, actor in enumerate(G.nodes):\n",
        "    if cluster_labels[i] not in actors_by_cluster:\n",
        "        actors_by_cluster[cluster_labels[i]] = []\n",
        "    actors_by_cluster[cluster_labels[i]].append(actor)\n",
        "\n",
        "# Print the actors in each cluster\n",
        "for cluster, actors in actors_by_cluster.items():\n",
        "    print(f\"Cluster {cluster}: {actors}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2MSO0-olPI_",
        "cell_id": "8a2fc1fa7b0e47fc975d4a5b31bd3778",
        "deepnote_cell_type": "markdown"
      },
      "source": [
        "## Kmeans\n",
        "Although primarily designed for vector data, K-means clustering can also be applied to graph data by transforming the graph into a feature matrix. Each node's features can be derived from its connectivity, attributes, or a combination of both."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1FJKs7vjr5p",
        "source_hash": null,
        "deepnote_to_be_reexecuted": true,
        "cell_id": "b1565ea5526340a1a7195269548f3d26",
        "deepnote_cell_type": "code"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Step 1: Preprocess the df and extract relevant columns (event_type, actor1, actor2)\n",
        "\n",
        "# Step 2: Create a graph\n",
        "G = nx.Graph()\n",
        "\n",
        "# Step 3: Add edges based on shared event types between actors\n",
        "for index, row in df.iterrows():\n",
        "    event_type = row['event_type']\n",
        "    actor1 = row['actor1']\n",
        "    actor2 = row['actor2']\n",
        "\n",
        "    # Add edge between actor1 and actor2 with the event_type as the weight\n",
        "    if actor1 != actor2:\n",
        "        G.add_edge(actor1, actor2, weight=event_type)\n",
        "\n",
        "# Step 4: Convert event types to numeric values\n",
        "label_encoder = LabelEncoder()\n",
        "df['event_type_numeric'] = label_encoder.fit_transform(df['event_type'])\n",
        "\n",
        "# Step 5: Create a feature matrix with numeric event types\n",
        "nodes = list(G.nodes)\n",
        "num_nodes = len(nodes)\n",
        "feature_matrix = np.zeros((num_nodes, num_nodes))\n",
        "for i in range(num_nodes):\n",
        "    for j in range(num_nodes):\n",
        "        if i != j:\n",
        "            actor1 = nodes[i]\n",
        "            actor2 = nodes[j]\n",
        "            if G.has_edge(actor1, actor2):\n",
        "                event_type_numeric = df.loc[(df['actor1'] == actor1) & (df['actor2'] == actor2), 'event_type_numeric'].values\n",
        "                if len(event_type_numeric) > 0:\n",
        "                    feature_matrix[i, j] = event_type_numeric[0]\n",
        "\n",
        "# Step 6: Apply K-means Clustering\n",
        "kmeans = KMeans(n_clusters=4)\n",
        "kmeans.fit(feature_matrix)\n",
        "\n",
        "# Step 7: Get the cluster labels and sort actors accordingly\n",
        "cluster_labels = kmeans.labels_\n",
        "actors = list(G.nodes)\n",
        "actors_sorted = [actor for _, actor in sorted(zip(cluster_labels, actors))]\n",
        "cluster_labels_sorted = sorted(cluster_labels)\n",
        "\n",
        "# Create a dictionary to store actors by cluster\n",
        "actors_by_cluster = {}\n",
        "for actor, cluster_label in zip(actors_sorted, cluster_labels_sorted):\n",
        "    if cluster_label not in actors_by_cluster:\n",
        "        actors_by_cluster[cluster_label] = []\n",
        "    actors_by_cluster[cluster_label].append(actor)\n",
        "\n",
        "# Convert dictionary values to lists\n",
        "actors_clusters = list(actors_by_cluster.values())\n",
        "\n",
        "# Print the actors belonging to each cluster\n",
        "for cluster in actors_clusters:\n",
        "    print(cluster)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTK9OMiJnNdr",
        "cell_id": "1a32537d4f324245bc4e309dba05b08e",
        "deepnote_cell_type": "markdown"
      },
      "source": [
        "## markov_clustering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mG_ci1PSj05o",
        "source_hash": null,
        "deepnote_to_be_reexecuted": true,
        "cell_id": "60529f87ab7c48b6a1c9d86af3b3a5a0",
        "deepnote_cell_type": "code"
      },
      "source": [
        "!pip install markov_clustering"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3unpyoJpjx0o",
        "source_hash": null,
        "deepnote_to_be_reexecuted": true,
        "cell_id": "b9072e98e88746a1a209c5f0656afb1e",
        "deepnote_cell_type": "code"
      },
      "source": [
        "import networkx as nx\n",
        "import numpy as np\n",
        "import markov_clustering as mc\n",
        "\n",
        "# Step 1: Preprocess the df and extract relevant columns (event_type, actor1, actor2)\n",
        "\n",
        "# Step 2: Create a graph\n",
        "G = nx.Graph()\n",
        "\n",
        "# Step 3: Add edges based on shared event types between actors\n",
        "for index, row in df.iterrows():\n",
        "    event_type = row['event_type']\n",
        "    actor1 = row['actor1']\n",
        "    actor2 = row['actor2']\n",
        "\n",
        "    # Add edge between actor1 and actor2 with the event_type as the weight\n",
        "    if actor1 != actor2:\n",
        "        G.add_edge(actor1, actor2, weight=event_type)\n",
        "\n",
        "# Step 4: Convert event types to numeric values\n",
        "label_encoder = LabelEncoder()\n",
        "df['event_type_numeric'] = label_encoder.fit_transform(df['event_type'])\n",
        "\n",
        "# Step 5: Create an adjacency matrix with numeric event types\n",
        "nodes = list(G.nodes)\n",
        "num_nodes = len(nodes)\n",
        "adj_matrix = np.zeros((num_nodes, num_nodes))\n",
        "for i in range(num_nodes):\n",
        "    for j in range(num_nodes):\n",
        "        if i != j:\n",
        "            actor1 = nodes[i]\n",
        "            actor2 = nodes[j]\n",
        "            if G.has_edge(actor1, actor2):\n",
        "                event_type_numeric = df.loc[(df['actor1'] == actor1) & (df['actor2'] == actor2), 'event_type_numeric'].values\n",
        "                if len(event_type_numeric) > 0:\n",
        "                    adj_matrix[i, j] = event_type_numeric[0]\n",
        "\n",
        "# Step 6: Apply Markov Clustering Algorithm (MCL)\n",
        "result = mc.run_mcl(adj_matrix)\n",
        "\n",
        "# Step 7: Get the clusters from the result\n",
        "clusters = mc.get_clusters(result)\n",
        "\n",
        "# Print the actors belonging to each cluster\n",
        "for i, cluster in enumerate(clusters):\n",
        "    print(f\"Cluster {i+1}:\")\n",
        "    for node_index in cluster:\n",
        "        actor = nodes[node_index]\n",
        "        print(f\"- {actor}\")\n",
        "    print()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3vsfz1VrkFj",
        "cell_id": "ae389a5f5cbe4a9e89d521ff4c399608",
        "deepnote_cell_type": "markdown"
      },
      "source": [
        "# Trying ensemble learning\n",
        "\n",
        "\n",
        "*   First Find individual clusters\n",
        "*   Then Ensemble the result to make cluster\n",
        "\n",
        "*  Egdes represents actors interaction in events\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Cluster-Ensembles: Cluster-Ensembles combines the results of multiple base clustering algorithms by considering each base clustering as a partition matrix. It uses an optimization algorithm to find a consensus partition matrix that maximizes the agreement with the individual base clustering results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrT1YU6vrw59",
        "source_hash": null,
        "deepnote_to_be_reexecuted": true,
        "cell_id": "925909abfe284ef099c2c9729eaa546d",
        "deepnote_cell_type": "code"
      },
      "source": [
        "import networkx as nx\n",
        "import numpy as np\n",
        "from sklearn.cluster import SpectralClustering, KMeans\n",
        "import markov_clustering as mc\n",
        "from collections import defaultdict\n",
        "\n",
        "# Step 1: Preprocess the df and extract relevant columns (event_type, actor1, actor2)\n",
        "\n",
        "# Step 2: Create a graph\n",
        "G = nx.Graph()\n",
        "\n",
        "# Step 3: Add edges based on shared event types between actors\n",
        "for index, row in df.iterrows():\n",
        "    event_type = row['event_type']\n",
        "    actor1 = row['actor1']\n",
        "    actor2 = row['actor2']\n",
        "\n",
        "    # Add edge between actor1 and actor2 with the event_type as the weight\n",
        "    if actor1 != actor2:\n",
        "        G.add_edge(actor1, actor2, weight=event_type)\n",
        "\n",
        "# Step 4: Convert event types to numeric values\n",
        "event_types = df['event_type'].unique()\n",
        "event_type_mapping = {event_type: i for i, event_type in enumerate(event_types)}\n",
        "\n",
        "# Step 5: Create a feature matrix with numeric event types\n",
        "nodes = list(G.nodes)\n",
        "num_nodes = len(nodes)\n",
        "feature_matrix = np.zeros((num_nodes, num_nodes))\n",
        "for i in range(num_nodes):\n",
        "    for j in range(num_nodes):\n",
        "        if i != j:\n",
        "            actor1 = nodes[i]\n",
        "            actor2 = nodes[j]\n",
        "            if G.has_edge(actor1, actor2):\n",
        "                matching_rows = df[(df['actor1'] == actor1) & (df['actor2'] == actor2)]\n",
        "                if not matching_rows.empty:\n",
        "                    event_type = matching_rows['event_type'].values[0]\n",
        "                    feature_matrix[i, j] = event_type_mapping[event_type]\n",
        "\n",
        "\n",
        "# Step 6: Apply individual clustering methods\n",
        "individual_results = []\n",
        "# Apply Spectral Clustering\n",
        "sc = SpectralClustering(n_clusters=4, affinity='precomputed', assign_labels='discretize')\n",
        "sc_labels = sc.fit_predict(feature_matrix)\n",
        "individual_results.append(sc_labels)\n",
        "# Apply K-means Clustering\n",
        "kmeans = KMeans(n_clusters=4)\n",
        "kmeans.fit(feature_matrix)\n",
        "kmeans_labels = kmeans.labels_\n",
        "individual_results.append(kmeans_labels)\n",
        "# Apply Markov Clustering Algorithm (MCL)\n",
        "result = mc.run_mcl(feature_matrix)\n",
        "mcl_clusters = mc.get_clusters(result)\n",
        "individual_results.append(mcl_clusters)\n",
        "\n",
        "# Step 7: Define the ensemble method (majority voting)\n",
        "def ensemble_majority_voting(cluster_labels):\n",
        "    ensemble_clusters = defaultdict(list)\n",
        "    for i, individual_result in enumerate(cluster_labels):\n",
        "        for actor, cluster_label in enumerate(individual_result):\n",
        "            ensemble_clusters[actor].append(cluster_label)\n",
        "\n",
        "    ensemble_result = []\n",
        "    for actor, cluster_labels in ensemble_clusters.items():\n",
        "        cluster_counts = defaultdict(int)\n",
        "        for cluster_label in cluster_labels:\n",
        "            cluster_counts[cluster_label] += 1\n",
        "        majority_cluster = max(cluster_counts, key=cluster_counts.get)\n",
        "        ensemble_result.append(majority_cluster)\n",
        "\n",
        "    return ensemble_result\n",
        "\n",
        "# Step 8: Apply the ensemble method to get the final clustering result\n",
        "ensemble_result = ensemble_majority_voting(individual_results)\n",
        "\n",
        "# Step 9: Print the ensemble clustering result\n",
        "print(\"Ensemble Clustering Result:\")\n",
        "for i, cluster in enumerate(set(ensemble_result)):\n",
        "    actors_in_cluster = [actor for actor, cluster_label in enumerate(ensemble_result) if cluster_label == cluster]\n",
        "    print(f\"Cluster {i+1}: {actors_in_cluster}\")\n",
        "\n",
        "\n",
        "# Create a dictionary to map actor indices to names\n",
        "actor_mapping = {i: actor for i, actor in enumerate(G.nodes)}\n",
        "\n",
        "# Print the ensemble clustering result with actor names\n",
        "print(\"Ensemble Clustering Result:\")\n",
        "for i, cluster in enumerate(set(ensemble_result)):\n",
        "    actors_in_cluster = [actor_mapping[actor] for actor, cluster_label in enumerate(ensemble_result) if cluster_label == cluster]\n",
        "    print(f\"Cluster {i+1}: {actors_in_cluster}\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-V5A8QXt-VP",
        "source_hash": null,
        "deepnote_to_be_reexecuted": true,
        "cell_id": "98a50670524c45de97f9788d0573d070",
        "deepnote_cell_type": "code"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "from sklearn.cluster import SpectralClustering, KMeans\n",
        "import markov_clustering as mc\n",
        "from collections import defaultdict\n",
        "\n",
        "# Step 1: Preprocess the df and extract relevant columns (event_type, actor1, actor2)\n",
        "\n",
        "# Step 2: Create a graph\n",
        "G = nx.Graph()\n",
        "\n",
        "# Step 3: Add edges based on shared event types between actors\n",
        "for index, row in df.iterrows():\n",
        "    event_type = row['event_type']\n",
        "    actor1 = row['actor1']\n",
        "    actor2 = row['actor2']\n",
        "\n",
        "    # Add edge between actor1 and actor2 with the event_type as the weight\n",
        "    if actor1 != actor2:\n",
        "        G.add_edge(actor1, actor2, weight=event_type)\n",
        "\n",
        "# Step 4: Convert event types to numeric values\n",
        "event_types = df['event_type'].unique()\n",
        "event_type_mapping = {event_type: i for i, event_type in enumerate(event_types)}\n",
        "\n",
        "# Step 5: Create a feature matrix with numeric event types\n",
        "nodes = list(G.nodes)\n",
        "num_nodes = len(nodes)\n",
        "feature_matrix = np.zeros((num_nodes, num_nodes))\n",
        "for i in range(num_nodes):\n",
        "    for j in range(num_nodes):\n",
        "        if i != j:\n",
        "            actor1 = nodes[i]\n",
        "            actor2 = nodes[j]\n",
        "            if G.has_edge(actor1, actor2):\n",
        "                matching_rows = df[(df['actor1'] == actor1) & (df['actor2'] == actor2)]\n",
        "                if not matching_rows.empty:\n",
        "                    event_type = matching_rows['event_type'].values[0]\n",
        "                    event_type_numeric = event_type_mapping.get(event_type)\n",
        "                    feature_matrix[i, j] = event_type_numeric if event_type_numeric is not None else 0\n",
        "\n",
        "# Step 6: Apply individual clustering methods\n",
        "individual_results = []\n",
        "# Apply Spectral Clustering\n",
        "sc = SpectralClustering(n_clusters=4, affinity='precomputed', assign_labels='discretize')\n",
        "sc_labels = sc.fit_predict(feature_matrix)\n",
        "individual_results.append(sc_labels)\n",
        "# Apply K-means Clustering\n",
        "kmeans = KMeans(n_clusters=4)\n",
        "kmeans.fit(feature_matrix)\n",
        "kmeans_labels = kmeans.labels_\n",
        "individual_results.append(kmeans_labels)\n",
        "# Apply Markov Clustering Algorithm (MCL)\n",
        "result = mc.run_mcl(feature_matrix)\n",
        "mcl_clusters = mc.get_clusters(result)\n",
        "individual_results.append(mcl_clusters)\n",
        "\n",
        "# Step 7: Define the ensemble method (majority voting)\n",
        "def ensemble_majority_voting(cluster_labels):\n",
        "    ensemble_clusters = defaultdict(list)\n",
        "    for i, individual_result in enumerate(cluster_labels):\n",
        "        for actor, cluster_label in enumerate(individual_result):\n",
        "            ensemble_clusters[actor].append(cluster_label)\n",
        "\n",
        "    ensemble_result = []\n",
        "    for actor, cluster_labels in ensemble_clusters.items():\n",
        "        cluster_counts = defaultdict(int)\n",
        "        for cluster_label in cluster_labels:\n",
        "            cluster_counts[cluster_label] += 1\n",
        "        majority_cluster = max(cluster_counts, key=cluster_counts.get)\n",
        "        ensemble_result.append(majority_cluster)\n",
        "\n",
        "    return ensemble_result\n",
        "\n",
        "# Step 8: Apply the ensemble method to get the final clustering result\n",
        "ensemble_result = ensemble_majority_voting(individual_results)\n",
        "\n",
        "# Step 9: Print the ensemble clustering result\n",
        "print(\"Ensemble Clustering Result:\")\n",
        "for i, cluster in enumerate(set(ensemble_result)):\n",
        "    actors_in_cluster = [actor for actor, cluster_label in enumerate(ensemble_result) if cluster_label == cluster]\n",
        "    print(f\"Cluster {i+1}: {actors_in_cluster}\")\n",
        "\n",
        "# Create a dictionary to map actor indices to names\n",
        "actor_mapping = {i: actor for i, actor in enumerate(G.nodes)}\n",
        "\n",
        "# Print the clusters with actor names\n",
        "print(\"Ensemble Clustering Result with Actor Names:\")\n",
        "for i, cluster in enumerate(set(ensemble_result)):\n",
        "    cluster_actors = [actor_mapping[actor_index] for actor_index, cluster_label in enumerate(ensemble_result) if cluster_label == cluster]\n",
        "    print(f\"Cluster {i+1}: {cluster_actors}\")\n",
        "\n",
        "\n",
        "# Visualize individual clustering results\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Spectral Clustering\n",
        "plt.subplot(1, 3, 1)\n",
        "pos = nx.circular_layout(G)\n",
        "colors = individual_results[0]\n",
        "nx.draw_networkx(G, pos, node_color=colors, with_labels=True, font_size=8)\n",
        "plt.title('Spectral Clustering')\n",
        "\n",
        "# K-means Clustering\n",
        "plt.subplot(1, 3, 2)\n",
        "colors = individual_results[1]\n",
        "nx.draw_networkx(G, pos, node_color=colors, with_labels=True, font_size=8)\n",
        "plt.title('K-means Clustering')\n",
        "\n",
        "# Markov Clustering\n",
        "plt.subplot(1, 3, 3)\n",
        "colors = [label for cluster in individual_results[2] for label in cluster]\n",
        "nx.draw_networkx(G, pos, node_color=colors, with_labels=True, font_size=8)\n",
        "plt.title('Markov Clustering')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Visualize the ensemble clustering result\n",
        "plt.figure(figsize=(8, 4))\n",
        "colors = ensemble_result\n",
        "nx.draw_networkx(G, pos, node_color=colors, with_labels=True, font_size=8)\n",
        "plt.title('Ensemble Clustering Result')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aG_X8U3BU8M7",
        "cell_id": "bda9bb01f8504a9e99151f816883b697",
        "deepnote_cell_type": "markdown"
      },
      "source": [
        "## Finding Eccentricity of the clusters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gncODEKoU9Uy",
        "source_hash": null,
        "deepnote_to_be_reexecuted": true,
        "cell_id": "194aeaeef9a848519370030f86bd176b",
        "deepnote_cell_type": "code"
      },
      "source": [
        "eccentricities = nx.eccentricity(G)\n",
        "print(eccentricities)\n",
        "diameter = max(eccentricities.values())\n",
        "print(f'diameter:{diameter}')\n",
        "radius = min(eccentricities.values())\n",
        "print(f'radius:{radius}')\n",
        "\n",
        "#visualization\n",
        "plt.figure(figsize=(8, 4))\n",
        "colors = [eccentricities[node] for node in G.nodes]\n",
        "nx.draw_networkx(G, pos, node_color=colors, with_labels=True, font_size=8)\n",
        "plt.title('Eccentricity')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPr8HQMW1IeH",
        "source_hash": null,
        "deepnote_to_be_reexecuted": true,
        "cell_id": "a55a4ce107694e229837fd946a9890f2",
        "deepnote_cell_type": "code"
      },
      "source": [
        "# Visualize individual clustering results\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Spectral Clustering\n",
        "plt.subplot(1, 3, 1)\n",
        "pos = nx.circular_layout(G)\n",
        "colors = individual_results[0]\n",
        "edge_labels = nx.get_edge_attributes(G, 'weight')\n",
        "nx.draw_networkx(G, pos, node_color=colors, with_labels=True, font_size=8)\n",
        "nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)\n",
        "plt.title('Spectral Clustering')\n",
        "\n",
        "# K-means Clustering\n",
        "plt.subplot(1, 3, 2)\n",
        "colors = individual_results[1]\n",
        "edge_labels = nx.get_edge_attributes(G, 'weight')\n",
        "nx.draw_networkx(G, pos, node_color=colors, with_labels=True, font_size=8)\n",
        "nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)\n",
        "plt.title('K-means Clustering')\n",
        "\n",
        "# Markov Clustering\n",
        "plt.subplot(1, 3, 3)\n",
        "colors = [label for cluster in individual_results[2] for label in cluster]\n",
        "edge_labels = nx.get_edge_attributes(G, 'weight')\n",
        "nx.draw_networkx(G, pos, node_color=colors, with_labels=True, font_size=8)\n",
        "nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)\n",
        "plt.title('Markov Clustering')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Visualize the ensemble clustering result\n",
        "plt.figure(figsize=(16, 10))\n",
        "colors = ensemble_result\n",
        "edge_labels = nx.get_edge_attributes(G, 'weight')\n",
        "nx.draw_networkx(G, pos, node_color=colors, with_labels=True, font_size=8)\n",
        "nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)\n",
        "plt.title('Ensemble Clustering Result')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iT0mvHOnWIit",
        "source_hash": null,
        "deepnote_to_be_reexecuted": true,
        "cell_id": "700aa4db5dfc4cf881ad803752e8991c",
        "deepnote_cell_type": "code"
      },
      "source": [
        "eccentricities = nx.eccentricity(G)\n",
        "print(eccentricities)\n",
        "diameter = max(eccentricities.values())\n",
        "print(f'diameter:{diameter}')\n",
        "radius = min(eccentricities.values())\n",
        "print(f'radius:{radius}')\n",
        "\n",
        "#visualization\n",
        "plt.figure(figsize=(8, 4))\n",
        "colors = [eccentricities[node] for node in G.nodes]\n",
        "nx.draw_networkx(G, pos, node_color=colors, with_labels=True, font_size=8)\n",
        "plt.title('Eccentricity')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BKpb3ry7BYQ",
        "cell_id": "814946536d8e4e3494f6a3f2cda35fc9",
        "deepnote_cell_type": "markdown"
      },
      "source": [
        "# Day 3 Finding other clusters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2E9jCqY277F",
        "source_hash": null,
        "deepnote_to_be_reexecuted": true,
        "cell_id": "0b719ed3cbcf4d129bc1b7058a5079f2",
        "deepnote_cell_type": "code"
      },
      "source": [
        "df.sample(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qE6bcyb18TcA",
        "cell_id": "de9777662f994e8c8be185f3507727c1",
        "deepnote_cell_type": "markdown"
      },
      "source": [
        "# IDEA 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DgYXXuQ7Iob",
        "source_hash": null,
        "deepnote_to_be_reexecuted": true,
        "cell_id": "9afe106400be4377a0e7a1d483b624f6",
        "deepnote_cell_type": "code"
      },
      "source": [
        "# i can find association rule of event_type,sub_event_type,actor1,actor2\n",
        "# what sub event happens when a event_type happen,\n",
        "# who are the actor2 if i know event_type,sub_event_type,actor1\n",
        "# This could lead policy makers fo find the unknown group"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0GqOqLcMhKL",
        "source_hash": null,
        "deepnote_to_be_reexecuted": true,
        "cell_id": "541edd496f0a48f78dc26c7f923810fb",
        "deepnote_cell_type": "code"
      },
      "source": [
        "# import pandas as pd\n",
        "# import networkx as nx\n",
        "# from mlxtend.preprocessing import TransactionEncoder\n",
        "# from mlxtend.frequent_patterns import apriori, association_rules\n",
        "\n",
        "# # Prepare the data\n",
        "# data = df[['event_type', 'sub_event_type', 'actor1', 'actor2']]\n",
        "# data = data.dropna()  # Remove rows with missing values\n",
        "# transactions = data.values.tolist()\n",
        "\n",
        "# # Convert the transactions into a binary encoded format\n",
        "# te = TransactionEncoder()\n",
        "# te_ary = te.fit(transactions).transform(transactions)\n",
        "# binary_data = pd.DataFrame(te_ary, columns=te.columns_)\n",
        "\n",
        "# # Apply the Apriori algorithm to find frequent itemsets\n",
        "# frequent_itemsets = apriori(binary_data, min_support=0.1, use_colnames=True)\n",
        "\n",
        "# # Generate association rules\n",
        "# association_rules_df = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.7)\n",
        "\n",
        "# # Display the association rules\n",
        "# print(association_rules_df)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xERQhZU1OHvn",
        "source_hash": null,
        "deepnote_to_be_reexecuted": true,
        "cell_id": "d8d8b75bc7be4dcfa78584d97cb2aa46",
        "deepnote_cell_type": "code"
      },
      "source": [
        "import pandas as pd\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "from mlxtend.frequent_patterns import apriori, association_rules\n",
        "\n",
        "\n",
        "# Select columns of interest\n",
        "cols = ['event_type', 'sub_event_type']\n",
        "df = df[cols]\n",
        "\n",
        "# Convert the data into transactions\n",
        "te = TransactionEncoder()\n",
        "te_ary = te.fit_transform(df.values)\n",
        "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
        "\n",
        "# Compute frequent itemsets using Apriori algorithm\n",
        "frequent_itemsets = apriori(df, min_support=0.01, use_colnames=True)\n",
        "\n",
        "# Generate association rules\n",
        "rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1)\n",
        "\n",
        "# Filter the rules by event_type and sub_event_type\n",
        "rules = rules[(rules['antecedents'] == {'event_type=XXX'}) & (rules['consequents'].str.contains('sub_event_type'))]\n",
        "\n",
        "# Print the resulting sub_event_types\n",
        "print(rules['consequents'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SW2kXKlc8XQu",
        "cell_id": "90c75f2c079f4a46b59c6854850d0bf3",
        "deepnote_cell_type": "markdown"
      },
      "source": [
        "# IDEA 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWw_61bd8bOX",
        "source_hash": null,
        "deepnote_to_be_reexecuted": true,
        "cell_id": "3ca98f4556c9419fa6eb829d9c50a2dc",
        "deepnote_cell_type": "code"
      },
      "source": [
        "# As i already made the clusters for actors, i can find the admin1 area\n",
        "# for each of the clusters. It will tell which areas have the actors and\n",
        "# thus helps law enforcement to take important decisions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8SwTEYiBrZr",
        "source_hash": null,
        "deepnote_to_be_reexecuted": true,
        "cell_id": "db644b9cb10e49efb236b0ad7a07d3fb",
        "deepnote_cell_type": "code"
      },
      "source": [
        "! pip install markov_clustering"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fhrkat7Q9Yhe",
        "source_hash": null,
        "deepnote_to_be_reexecuted": true,
        "cell_id": "72b3ad16228c44fc8552784b8bf5c835",
        "deepnote_cell_type": "code"
      },
      "source": [
        "import networkx as nx\n",
        "from networkx.algorithms.community import girvan_newman\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "from sklearn.cluster import SpectralClustering, KMeans\n",
        "import markov_clustering as mc\n",
        "from collections import defaultdict\n",
        "\n",
        "# Step 1: Preprocess the df and extract relevant columns (event_type, actor1, actor2)\n",
        "\n",
        "# Step 2: Create a graph\n",
        "G = nx.Graph()\n",
        "\n",
        "# Step 3: Add edges based on shared event types between actors\n",
        "for index, row in df.iterrows():\n",
        "    event_type = row['event_type']\n",
        "    actor1 = row['actor1']\n",
        "    actor2 = row['actor2']\n",
        "\n",
        "    # Add edge between actor1 and actor2 with the event_type as the weight\n",
        "    if actor1 != actor2:\n",
        "        G.add_edge(actor1, actor2, weight=event_type)\n",
        "\n",
        "# Step 4: Convert event types to numeric values\n",
        "event_types = df['event_type'].unique()\n",
        "event_type_mapping = {event_type: i for i, event_type in enumerate(event_types)}\n",
        "\n",
        "# Step 5: Create a feature matrix with numeric event types\n",
        "nodes = list(G.nodes)\n",
        "num_nodes = len(nodes)\n",
        "feature_matrix = np.zeros((num_nodes, num_nodes))\n",
        "for i in range(num_nodes):\n",
        "    for j in range(num_nodes):\n",
        "        if i != j:\n",
        "            actor1 = nodes[i]\n",
        "            actor2 = nodes[j]\n",
        "            if G.has_edge(actor1, actor2):\n",
        "                matching_rows = df[(df['actor1'] == actor1) & (df['actor2'] == actor2)]\n",
        "                if not matching_rows.empty:\n",
        "                    event_type = matching_rows['event_type'].values[0]\n",
        "                    event_type_numeric = event_type_mapping.get(event_type)\n",
        "                    feature_matrix[i, j] = event_type_numeric if event_type_numeric is not None else 0\n",
        "\n",
        "# Step 6: Apply individual clustering methods\n",
        "individual_results = []\n",
        "# Apply Spectral Clustering\n",
        "sc = SpectralClustering(n_clusters=4, affinity='precomputed', assign_labels='discretize')\n",
        "sc_labels = sc.fit_predict(feature_matrix)\n",
        "individual_results.append(sc_labels)\n",
        "# Apply K-means Clustering\n",
        "kmeans = KMeans(n_clusters=4)\n",
        "kmeans.fit(feature_matrix)\n",
        "kmeans_labels = kmeans.labels_\n",
        "individual_results.append(kmeans_labels)\n",
        "# Apply Markov Clustering Algorithm (MCL)\n",
        "result = mc.run_mcl(feature_matrix)\n",
        "mcl_clusters = mc.get_clusters(result)\n",
        "individual_results.append(mcl_clusters)\n",
        "\n",
        "# Step 7: Define the ensemble method (majority voting)\n",
        "def ensemble_majority_voting(cluster_labels):\n",
        "    ensemble_clusters = defaultdict(list)\n",
        "    for i, individual_result in enumerate(cluster_labels):\n",
        "        for actor, cluster_label in enumerate(individual_result):\n",
        "            ensemble_clusters[actor].append(cluster_label)\n",
        "\n",
        "    ensemble_result = []\n",
        "    for actor, cluster_labels in ensemble_clusters.items():\n",
        "        cluster_counts = defaultdict(int)\n",
        "        for cluster_label in cluster_labels:\n",
        "            cluster_counts[cluster_label] += 1\n",
        "        majority_cluster = max(cluster_counts, key=cluster_counts.get)\n",
        "        ensemble_result.append(majority_cluster)\n",
        "\n",
        "    return ensemble_result\n",
        "\n",
        "# Step 8: Apply the ensemble method to get the final clustering result\n",
        "ensemble_result = ensemble_majority_voting(individual_results)\n",
        "\n",
        "# Step 9: Print the ensemble clustering result\n",
        "print(\"Ensemble Clustering Result:\")\n",
        "for i, cluster in enumerate(set(ensemble_result)):\n",
        "    actors_in_cluster = [actor for actor, cluster_label in enumerate(ensemble_result) if cluster_label == cluster]\n",
        "    print(f\"Cluster {i+1}: {actors_in_cluster}\")\n",
        "\n",
        "# Create a dictionary to map actor indices to names\n",
        "actor_mapping = {i: actor for i, actor in enumerate(G.nodes)}\n",
        "\n",
        "# Print the clusters with actor names\n",
        "print(\"Ensemble Clustering Result with Actor Names:\")\n",
        "for i, cluster in enumerate(set(ensemble_result)):\n",
        "    cluster_actors = [actor_mapping[actor_index] for actor_index, cluster_label in enumerate(ensemble_result) if cluster_label == cluster]\n",
        "    print(f\"Cluster {i+1}: {cluster_actors}\")\n",
        "\n",
        "\n",
        "# import networkx as nx\n",
        "# from networkx.algorithms.community import girvan_newman\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# import networkx as nx\n",
        "# from networkx.algorithms.community import girvan_newman\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # Create a new graph\n",
        "# G = nx.Graph()\n",
        "\n",
        "# # Add nodes (admin1 areas)\n",
        "# admin1_areas = df['admin1'].unique()\n",
        "# G.add_nodes_from(admin1_areas)\n",
        "\n",
        "# # Create a mapping of actors to admin1 areas\n",
        "# actors_to_admin1s = {}\n",
        "# for index, row in df.iterrows():\n",
        "#     actor1 = row['actor1']\n",
        "#     actor2 = row['actor2']\n",
        "#     admin1 = row['admin1']\n",
        "\n",
        "#     if actor1 not in actors_to_admin1s:\n",
        "#         actors_to_admin1s[actor1] = []\n",
        "#     actors_to_admin1s[actor1].append(admin1)\n",
        "\n",
        "#     if actor2 not in actors_to_admin1s:\n",
        "#         actors_to_admin1s[actor2] = []\n",
        "#     actors_to_admin1s[actor2].append(admin1)\n",
        "\n",
        "# # Add edges between admin1 areas that share at least one actor\n",
        "# for i, cluster in enumerate(ensemble_result):\n",
        "#     for actor, admin1s in actors_to_admin1s.items():\n",
        "#         if cluster == ensemble_result[i]:  # Choose the clustering result from the desired algorithm\n",
        "#             for j in range(len(admin1s)):\n",
        "#                 for k in range(j+1, len(admin1s)):\n",
        "#                     G.add_edge(admin1s[j], admin1s[k], weight=1)\n",
        "\n",
        "# # Apply Girvan-Newman algorithm for community detection\n",
        "# communities_generator = girvan_newman(G)\n",
        "# top_level_communities = next(communities_generator)\n",
        "# communities = list(sorted(map(sorted, top_level_communities)))\n",
        "\n",
        "# # Visualize the clustering result\n",
        "# plt.figure(figsize=(8, 8))\n",
        "# pos = nx.spring_layout(G)\n",
        "# colors = [0] * len(G.nodes())\n",
        "# for i, community in enumerate(communities):\n",
        "#     for admin1 in community:\n",
        "#         colors[list(G.nodes()).index(admin1)] = i\n",
        "# nx.draw_networkx(G, pos, node_color=colors, with_labels=True, font_size=8)\n",
        "# plt.title('Admin1 Clustering Result')\n",
        "# plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "run4TRARsd_r",
        "cell_id": "7b51ed8d41a94f8db3fb70878ac15263",
        "deepnote_cell_type": "markdown"
      },
      "source": [
        "# Area Clustering based on average fatality rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6J-ABclsl-x",
        "source_hash": null,
        "deepnote_to_be_reexecuted": true,
        "cell_id": "e7c77bb635bb42c884dab01e86745bbc",
        "deepnote_cell_type": "code"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "from sklearn.cluster import SpectralClustering\n",
        "\n",
        "# Load data from a CSV file\n",
        "data = df\n",
        "\n",
        "# Filter the data to only include the relevant columns\n",
        "data = data[['admin1', 'fatalities']]\n",
        "\n",
        "# Group the data by admin1 and compute the average fatalities rate for each area\n",
        "grouped = data.groupby('admin1').mean()\n",
        "\n",
        "# Create a new graph\n",
        "G = nx.Graph()\n",
        "\n",
        "# Add nodes to the graph\n",
        "for admin1, fatalities in grouped.iterrows():\n",
        "    G.add_node(admin1, fatalities=fatalities['fatalities'])\n",
        "\n",
        "# Compute similarity between nodes (admin1 areas) based on the average fatalities rate\n",
        "similarity_matrix = np.zeros((len(grouped), len(grouped)))\n",
        "for i, (_, row1) in enumerate(grouped.iterrows()):\n",
        "    for j, (_, row2) in enumerate(grouped.iterrows()):\n",
        "        similarity_matrix[i, j] = np.abs(row1['fatalities'] - row2['fatalities'])\n",
        "\n",
        "# Perform spectral clustering on the graph\n",
        "sc = SpectralClustering(n_clusters=3, affinity='precomputed', random_state=42)\n",
        "labels = sc.fit_predict(similarity_matrix)\n",
        "\n",
        "# Add the cluster labels as a node attribute\n",
        "for i, admin1 in enumerate(grouped.index):\n",
        "    G.nodes[admin1]['cluster'] = labels[i]\n",
        "\n",
        "# Compute the average fatalities rate for each cluster\n",
        "cluster_fatalities = grouped.groupby(labels).mean()\n",
        "\n",
        "# Visualize the clusters in a bar chart\n",
        "fig, ax = plt.subplots()\n",
        "ax.barh(range(len(cluster_fatalities)), cluster_fatalities['fatalities'])\n",
        "ax.set_yticks(range(len(cluster_fatalities)))\n",
        "ax.set_yticklabels(['Cluster ' + str(i+1) for i in range(len(cluster_fatalities))])\n",
        "ax.set_xlabel('Average Fatalities Rate')\n",
        "ax.set_title('Clusters of Areas by Fatalities Rate')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMXUF4r3BszA",
        "source_hash": null,
        "deepnote_to_be_reexecuted": true,
        "cell_id": "afb286891c8d404eb07413a99761b4e0",
        "deepnote_cell_type": "code"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "from sklearn.cluster import SpectralClustering\n",
        "\n",
        "# Load data from a CSV file\n",
        "data = df\n",
        "\n",
        "# Filter the data to only include the relevant columns\n",
        "data = data[['admin1', 'fatalities']]\n",
        "\n",
        "# Group the data by admin1 and compute the average fatalities rate for each area\n",
        "grouped = data.groupby('admin1').mean()\n",
        "\n",
        "# Create a new graph\n",
        "G = nx.Graph()\n",
        "\n",
        "# Add nodes to the graph\n",
        "for admin1, fatalities in grouped.iterrows():\n",
        "    G.add_node(admin1, fatalities=fatalities['fatalities'])\n",
        "\n",
        "# Compute similarity between nodes (admin1 areas) based on the average fatalities rate\n",
        "similarity_matrix = np.zeros((len(grouped), len(grouped)))\n",
        "for i, (_, row1) in enumerate(grouped.iterrows()):\n",
        "    for j, (_, row2) in enumerate(grouped.iterrows()):\n",
        "        similarity_matrix[i, j] = np.abs(row1['fatalities'] - row2['fatalities'])\n",
        "\n",
        "# Perform spectral clustering on the graph\n",
        "sc = SpectralClustering(n_clusters=3, affinity='precomputed', random_state=42)\n",
        "labels = sc.fit_predict(similarity_matrix)\n",
        "\n",
        "# Add the cluster labels as a node attribute\n",
        "for i, admin1 in enumerate(grouped.index):\n",
        "    G.nodes[admin1]['cluster'] = labels[i]\n",
        "\n",
        "# Compute the average fatalities rate for each cluster\n",
        "grouped['cluster'] = labels\n",
        "cluster_fatalities = grouped.groupby('cluster').mean()\n",
        "\n",
        "# Visualize the areas in each cluster with the average fatality rate\n",
        "fig, axes = plt.subplots(1, len(cluster_fatalities), figsize=(10, 6))\n",
        "\n",
        "for i, (cluster, data) in enumerate(grouped.groupby('cluster')):\n",
        "    sizes = data['fatalities']\n",
        "    labels = data.index\n",
        "    explode = [0.05] * len(data)\n",
        "    colors = np.random.rand(len(data), 3)\n",
        "    axes[i].pie(sizes, labels=labels, explode=explode, colors=colors, autopct='%1.1f%%', startangle=90)\n",
        "    axes[i].set_aspect('equal')\n",
        "    axes[i].set_title('Cluster ' + str(cluster + 1))\n",
        "    plt.subplots_adjust(wspace=0.5)\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-qVAcx5xf9C",
        "cell_id": "be2ad18f2a864f7fa1cdc0103b114ab8",
        "deepnote_cell_type": "markdown"
      },
      "source": [
        "# With K_Means Clustering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BI8x7LpduS9z",
        "source_hash": null,
        "deepnote_to_be_reexecuted": true,
        "cell_id": "1be1967adef4477aa8fbe86f06e87430",
        "deepnote_cell_type": "code"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Load data from a CSV file\n",
        "data = df\n",
        "\n",
        "# Filter the data to only include the relevant columns\n",
        "data = data[['admin1', 'fatalities']]\n",
        "\n",
        "# Group the data by admin1 and compute the average fatalities rate for each area\n",
        "grouped = data.groupby('admin1').mean()\n",
        "\n",
        "# Create a new graph\n",
        "G = nx.Graph()\n",
        "\n",
        "# Add nodes to the graph\n",
        "for admin1, fatalities in grouped.iterrows():\n",
        "    G.add_node(admin1, fatalities=fatalities['fatalities'])\n",
        "\n",
        "# Compute the similarity matrix based on the average fatalities rate\n",
        "similarity_matrix = np.zeros((len(grouped), len(grouped)))\n",
        "for i, (_, row1) in enumerate(grouped.iterrows()):\n",
        "    for j, (_, row2) in enumerate(grouped.iterrows()):\n",
        "        similarity_matrix[i, j] = np.abs(row1['fatalities'] - row2['fatalities'])\n",
        "\n",
        "# Perform K-Means clustering on the graph nodes\n",
        "num_clusters = 3\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42).fit(similarity_matrix)\n",
        "\n",
        "# Add the cluster labels to the graph nodes\n",
        "for i, admin1 in enumerate(grouped.index):\n",
        "    G.nodes[admin1]['cluster'] = kmeans.labels_[i]\n",
        "\n",
        "# Compute the average fatalities rate for each cluster\n",
        "cluster_fatalities = grouped.groupby([G.nodes[n]['cluster'] for n in G.nodes()]).mean()\n",
        "\n",
        "# Visualize the clusters in a bar chart\n",
        "fig, ax = plt.subplots()\n",
        "ax.barh(range(len(cluster_fatalities)), cluster_fatalities['fatalities'])\n",
        "ax.set_yticks(range(len(cluster_fatalities)))\n",
        "ax.set_yticklabels(['Cluster ' + str(i+1) for i in range(len(cluster_fatalities))])\n",
        "ax.set_xlabel('Average Fatalities Rate')\n",
        "ax.set_title('Clusters of Areas by Fatalities Rate')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wbWLnIVwupx",
        "source_hash": null,
        "deepnote_to_be_reexecuted": true,
        "cell_id": "e94cf985dd824eb0850a58408d29b74c",
        "deepnote_cell_type": "code"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Load data from a CSV file\n",
        "data = df\n",
        "\n",
        "# Filter the data to only include the relevant columns\n",
        "data = data[['admin1', 'fatalities']]\n",
        "\n",
        "# Group the data by admin1 and compute the average fatalities rate for each area\n",
        "grouped = data.groupby('admin1').mean()\n",
        "\n",
        "# Create a new graph\n",
        "G = nx.Graph()\n",
        "\n",
        "# Add nodes to the graph\n",
        "for admin1, fatalities in grouped.iterrows():\n",
        "    G.add_node(admin1, fatalities=fatalities['fatalities'])\n",
        "\n",
        "# Compute similarity between nodes (admin1 areas) based on the average fatalities rate\n",
        "similarity_matrix = np.zeros((len(grouped), len(grouped)))\n",
        "for i, (_, row1) in enumerate(grouped.iterrows()):\n",
        "    for j, (_, row2) in enumerate(grouped.iterrows()):\n",
        "        similarity_matrix[i, j] = np.abs(row1['fatalities'] - row2['fatalities'])\n",
        "\n",
        "# Perform K-Means clustering on the graph nodes\n",
        "num_clusters = 3\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42).fit(similarity_matrix)\n",
        "\n",
        "# Add the cluster labels as a node attribute\n",
        "for i, admin1 in enumerate(grouped.index):\n",
        "    G.nodes[admin1]['cluster'] = kmeans.labels_[i]\n",
        "\n",
        "# Compute the average fatalities rate for each cluster\n",
        "grouped['cluster'] = kmeans.labels_\n",
        "cluster_fatalities = grouped.groupby('cluster').mean()\n",
        "\n",
        "# Visualize the areas in each cluster with the average fatality rate\n",
        "fig, axes = plt.subplots(1, len(cluster_fatalities), figsize=(10, 6))\n",
        "\n",
        "for i, (cluster, data) in enumerate(grouped.groupby('cluster')):\n",
        "    sizes = data['fatalities']\n",
        "    labels = data.index\n",
        "    explode = [0.05] * len(data)\n",
        "    colors = np.random.rand(len(data), 3)\n",
        "    axes[i].pie(sizes, labels=labels, explode=explode, colors=colors, autopct='%1.1f%%', startangle=90)\n",
        "    axes[i].set_aspect('equal')\n",
        "    axes[i].set_title('Cluster ' + str(cluster + 1))\n",
        "    plt.subplots_adjust(wspace=0.5)\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIW5rLLvzggy",
        "cell_id": "c2bf3597beda44288654774ce9923e93",
        "deepnote_cell_type": "markdown"
      },
      "source": [
        "# Time to see them together in action\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBDJtsmdzkvO",
        "source_hash": null,
        "deepnote_to_be_reexecuted": true,
        "cell_id": "84b2ac61f0c54c8ca47fecb63784a3b6",
        "deepnote_cell_type": "code"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "from sklearn.cluster import SpectralClustering, KMeans\n",
        "\n",
        "# Load data from a CSV file\n",
        "data = df\n",
        "\n",
        "# Filter the data to only include the relevant columns\n",
        "data = data[['admin1', 'fatalities']]\n",
        "\n",
        "# Group the data by admin1 and compute the average fatalities rate for each area\n",
        "grouped = data.groupby('admin1').mean()\n",
        "\n",
        "# Create a new graph\n",
        "G = nx.Graph()\n",
        "\n",
        "# Add nodes to the graph\n",
        "for admin1, fatalities in grouped.iterrows():\n",
        "    G.add_node(admin1, fatalities=fatalities['fatalities'])\n",
        "\n",
        "# Compute similarity between nodes (admin1 areas) based on the average fatalities rate\n",
        "similarity_matrix = np.zeros((len(grouped), len(grouped)))\n",
        "for i, (_, row1) in enumerate(grouped.iterrows()):\n",
        "    for j, (_, row2) in enumerate(grouped.iterrows()):\n",
        "        similarity_matrix[i, j] = np.abs(row1['fatalities'] - row2['fatalities'])\n",
        "\n",
        "# Perform Spectral Clustering on the graph\n",
        "sc = SpectralClustering(n_clusters=3, affinity='precomputed', random_state=42)\n",
        "labels_spectral = sc.fit_predict(similarity_matrix)\n",
        "\n",
        "# Add the cluster labels as a node attribute\n",
        "for i, admin1 in enumerate(grouped.index):\n",
        "    G.nodes[admin1]['cluster'] = labels_spectral[i]\n",
        "\n",
        "# Compute the average fatalities rate for each cluster\n",
        "grouped['cluster'] = labels_spectral\n",
        "cluster_fatalities_spectral = grouped.groupby('cluster').mean()\n",
        "\n",
        "# Perform K-Means Clustering on the graph nodes\n",
        "num_clusters = 3\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42).fit(similarity_matrix)\n",
        "\n",
        "# Add the cluster labels as a node attribute\n",
        "for i, admin1 in enumerate(grouped.index):\n",
        "    G.nodes[admin1]['cluster'] = kmeans.labels_[i]\n",
        "\n",
        "# Compute the average fatalities rate for each cluster\n",
        "grouped['cluster'] = kmeans.labels_\n",
        "cluster_fatalities_kmeans = grouped.groupby('cluster').mean()\n",
        "\n",
        "# Visualize the clusters with bar charts and pie charts\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
        "\n",
        "# Spectral Clustering - Bar Chart\n",
        "axes[0, 0].barh(range(len(cluster_fatalities_spectral)), cluster_fatalities_spectral['fatalities'])\n",
        "axes[0, 0].set_yticks(range(len(cluster_fatalities_spectral)))\n",
        "axes[0, 0].set_yticklabels(['Cluster ' + str(i + 1) + ': ' + str(round(rate, 2)) for i, rate in enumerate(cluster_fatalities_spectral['fatalities'])])\n",
        "axes[0, 0].set_xlabel('Average Fatalities Rate')\n",
        "axes[0, 0].set_title('Spectral Clustering - Bar Chart')\n",
        "\n",
        "# Spectral Clustering - Pie Chart\n",
        "for i, (cluster, data) in enumerate(grouped.groupby('cluster')):\n",
        "    sizes = data['fatalities']\n",
        "    labels = data.index\n",
        "    explode = [0.05] * len(data)\n",
        "    colors = np.random.rand(len(data), 3)\n",
        "    axes[1, 0].pie(sizes, labels=labels, explode=explode, colors=colors, autopct='%1.1f%%', startangle=90)\n",
        "    axes[1, 0].set_aspect('equal')\n",
        "    axes[1, 0].set_title('Spectral Clustering - Pie Chart')\n",
        "\n",
        "# K-Means Clustering - Bar Chart\n",
        "axes[0, 1].barh(range(len(cluster_fatalities_kmeans)), cluster_fatalities_kmeans['fatalities'])\n",
        "axes[0, 1].set_yticks(range(len(cluster_fatalities_kmeans)))\n",
        "axes[0, 1].set_yticklabels(['Cluster ' + str(i + 1) + ': ' + str(round(rate, 2)) for i, rate in enumerate(cluster_fatalities_kmeans['fatalities'])])\n",
        "axes[0, 1].set_xlabel('Average Fatalities Rate')\n",
        "axes[0, 1].set_title('K-Means Clustering - Bar Chart')\n",
        "\n",
        "# K-Means Clustering - Pie Chart\n",
        "for i, (cluster, data) in enumerate(grouped.groupby('cluster')):\n",
        "    sizes = data['fatalities']\n",
        "    labels = data.index\n",
        "    explode = [0.05] * len(data)\n",
        "    colors = np.random.rand(len(data), 3)\n",
        "    axes[1, 1].pie(sizes, labels=labels, explode=explode, colors=colors, autopct='%1.1f%%', startangle=90)\n",
        "    axes[1, 1].set_aspect('equal')\n",
        "    axes[1, 1].set_title('K-Means Clustering - Pie Chart')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvMDnvuv8O6Z",
        "source_hash": null,
        "deepnote_to_be_reexecuted": true,
        "cell_id": "eb9879155ec943d38a9ead04f5725a5e",
        "deepnote_cell_type": "code"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "from sklearn.cluster import SpectralClustering, KMeans\n",
        "\n",
        "# Load data from a CSV file\n",
        "data = df\n",
        "\n",
        "# Filter the data to only include the relevant columns\n",
        "data = data[['admin1', 'fatalities']]\n",
        "\n",
        "# Group the data by admin1 and compute the average fatalities rate for each area\n",
        "grouped = data.groupby('admin1').mean()\n",
        "\n",
        "# Create a new graph\n",
        "G = nx.Graph()\n",
        "\n",
        "# Add nodes to the graph\n",
        "for admin1, fatalities in grouped.iterrows():\n",
        "    G.add_node(admin1, fatalities=fatalities['fatalities'])\n",
        "\n",
        "# Compute similarity between nodes (admin1 areas) based on the average fatalities rate\n",
        "similarity_matrix = np.zeros((len(grouped), len(grouped)))\n",
        "for i, (_, row1) in enumerate(grouped.iterrows()):\n",
        "    for j, (_, row2) in enumerate(grouped.iterrows()):\n",
        "        similarity_matrix[i, j] = np.abs(row1['fatalities'] - row2['fatalities'])\n",
        "\n",
        "# Perform Spectral Clustering on the graph\n",
        "sc = SpectralClustering(n_clusters=3, affinity='precomputed', random_state=83)\n",
        "labels_spectral = sc.fit_predict(similarity_matrix)\n",
        "\n",
        "# Add the cluster labels as a node attribute\n",
        "for i, admin1 in enumerate(grouped.index):\n",
        "    G.nodes[admin1]['cluster_spectral'] = labels_spectral[i]\n",
        "\n",
        "# Compute the average fatalities rate for each cluster (Spectral Clustering)\n",
        "grouped['cluster_spectral'] = labels_spectral\n",
        "cluster_fatalities_spectral = grouped.groupby('cluster_spectral').mean()\n",
        "\n",
        "# Perform K-Means Clustering on the graph nodes\n",
        "num_clusters = 3\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=83).fit(similarity_matrix)\n",
        "\n",
        "# Add the cluster labels as a node attribute\n",
        "for i, admin1 in enumerate(grouped.index):\n",
        "    G.nodes[admin1]['cluster_kmeans'] = kmeans.labels_[i]\n",
        "\n",
        "# Compute the average fatalities rate for each cluster (K-Means Clustering)\n",
        "grouped['cluster_kmeans'] = kmeans.labels_\n",
        "cluster_fatalities_kmeans = grouped.groupby('cluster_kmeans').mean()\n",
        "\n",
        "# Visualize the clusters with pie charts\n",
        "fig, axes = plt.subplots(2, num_clusters, figsize=(12, 4))\n",
        "\n",
        "# Spectral Clustering - Pie Chart\n",
        "for i, (cluster, data) in enumerate(grouped.groupby('cluster_spectral')):\n",
        "    sizes = data['fatalities']\n",
        "    labels = data.index\n",
        "    explode = [0.05] * len(data)\n",
        "    colors = np.random.rand(len(data), 3)\n",
        "    axes[0, i].pie(sizes, labels=labels, explode=explode, textprops={'fontsize': 8}, colors=colors, autopct='%1.1f%%', startangle=90)\n",
        "    axes[0, i].set_aspect('equal')\n",
        "    axes[0, i].set_title(f'Spectral Clustering - Cluster {cluster+1}\\nFatality Rate: {cluster_fatalities_spectral.loc[cluster, \"fatalities\"]:.2f}', fontsize=8)\n",
        "\n",
        "# K-Means Clustering - Pie Chart\n",
        "for i, (cluster, data) in enumerate(grouped.groupby('cluster_kmeans')):\n",
        "    sizes = data['fatalities']\n",
        "    labels = data.index\n",
        "    explode = [0.05] * len(data)\n",
        "    colors = np.random.rand(len(data), 3)\n",
        "    axes[1, i].pie(sizes, labels=labels, explode=explode, textprops={'fontsize': 8}, colors=colors, autopct='%1.1f%%', startangle=90)\n",
        "    axes[1, i].set_aspect('equal')\n",
        "    axes[1, i].set_title(f'K-Means Clustering - Cluster {cluster+1}\\nFatality Rate: {cluster_fatalities_kmeans.loc[cluster, \"fatalities\"]:.2f}', fontsize=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywLG8AX--hQO",
        "cell_id": "082b1c2ef1444b38a81ce3cdec500483",
        "deepnote_cell_type": "markdown"
      },
      "source": [
        "# Now i should find which actors are related in each cluster. (Not getting good result)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4yMcdxA_O2j",
        "source_hash": null,
        "deepnote_to_be_reexecuted": true,
        "cell_id": "b3f1a6803dba4a589000d20ca9b25040",
        "deepnote_cell_type": "code"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Load data from a CSV file\n",
        "data = df\n",
        "\n",
        "# Filter the data to only include the relevant columns\n",
        "data = data[['admin1', 'fatalities', 'actor1', 'actor2']]\n",
        "\n",
        "# Group the data by admin1 and compute the average fatalities rate for each area\n",
        "grouped = data.groupby('admin1').mean()\n",
        "\n",
        "# Create a new graph\n",
        "G = nx.Graph()\n",
        "\n",
        "# Add nodes to the graph\n",
        "for admin1, fatalities in grouped.iterrows():\n",
        "    G.add_node(admin1, fatalities=fatalities['fatalities'])\n",
        "\n",
        "# Compute similarity between nodes (admin1 areas) based on the average fatalities rate\n",
        "similarity_matrix = np.zeros((len(grouped), len(grouped)))\n",
        "for i, (_, row1) in enumerate(grouped.iterrows()):\n",
        "    for j, (_, row2) in enumerate(grouped.iterrows()):\n",
        "        similarity_matrix[i, j] = np.abs(row1['fatalities'] - row2['fatalities'])\n",
        "\n",
        "# Perform K-Means Clustering on the graph nodes\n",
        "num_clusters = 5\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=83).fit(similarity_matrix)\n",
        "\n",
        "# Add the cluster labels as a node attribute\n",
        "for i, admin1 in enumerate(grouped.index):\n",
        "    G.nodes[admin1]['cluster_kmeans'] = kmeans.labels_[i]\n",
        "\n",
        "# Compute the average fatalities rate for each cluster (K-Means Clustering)\n",
        "grouped['cluster_kmeans'] = kmeans.labels_\n",
        "cluster_fatalities_kmeans = grouped.groupby('cluster_kmeans').mean()\n",
        "\n",
        "# Visualize the clusters with pie charts\n",
        "fig, axes = plt.subplots(1, num_clusters, figsize=(8, 4))\n",
        "\n",
        "# K-Means Clustering - Pie Chart\n",
        "for i, (cluster, data) in enumerate(grouped.groupby('cluster_kmeans')):\n",
        "    sizes = data['fatalities']\n",
        "    labels = data.index\n",
        "    explode = [0.05] * len(data)\n",
        "    colors = np.random.rand(len(data), 3)\n",
        "    axes[i].pie(sizes, labels=labels, explode=explode, textprops={'fontsize': 8}, colors=colors, autopct='%1.1f%%', startangle=90)\n",
        "    axes[i].set_aspect('equal')\n",
        "    axes[i].set_title(f'K-Means Clustering - Cluster {cluster+1}\\nFatality Rate: {cluster_fatalities_kmeans.loc[cluster, \"fatalities\"]:.2f}', fontsize=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Find related actors in each cluster of K-Means Clustering\n",
        "for cluster in range(num_clusters):\n",
        "    print(f\"Cluster {cluster+1} - Related Actors:\")\n",
        "    cluster_labels = grouped[grouped['cluster_kmeans'] == cluster].index\n",
        "    cluster_actors = set(df.loc[df['admin1'].isin(cluster_labels), 'actor1']).union(set(df.loc[df['admin1'].isin(cluster_labels), 'actor2']))\n",
        "    print(cluster_actors)\n",
        "    print()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-o8V96Vcd3Is",
        "cell_id": "13bea1e8a2a341bdbfcc44f0d2e62fd2",
        "deepnote_cell_type": "markdown"
      },
      "source": [
        "# cluster based on event_type based on fatality rate and finding actors involved"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2Y610sMfvFI",
        "source_hash": null,
        "deepnote_to_be_reexecuted": true,
        "cell_id": "ae50e5e6765d4ab4917d510c32247002",
        "deepnote_cell_type": "code"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Datasets/Armed_conflict_2001-01-01-2021-11-01-South_Asia-Bangladesh.csv')\n",
        "\n",
        "remove_column=['data_id','iso','event_id_cnty','event_id_no_cnty','time_precision','notes','timestamp','iso3','region','country',\n",
        "               'geo_precision','source','source_scale','notes','interaction', 'assoc_actor_1','assoc_actor_2','inter1','inter2'\n",
        "               ]\n",
        "df.drop(columns=remove_column,inplace=True)\n",
        "\n",
        "from datetime import datetime\n",
        "month=[]\n",
        "\n",
        "for date_str in df['event_date']:\n",
        "  date_object = datetime.strptime(date_str, '%d %B %Y').date()\n",
        "  month.append(date_object.month)\n",
        "\n",
        "df['month']=month\n",
        "\n",
        "\n",
        "df.drop(columns=['event_date'],inplace=True)\n",
        "my_column = df.pop('month')\n",
        "df.insert(3, 'month', my_column)\n",
        "my_column = df.pop('month')\n",
        "df.insert(1, 'month', my_column)\n",
        "\n",
        "df = df.replace(np.nan, \"unknown\")\n",
        "\n",
        "# filter actor1s having count less than 124 to \"other\"\n",
        "counts = df['actor1'].value_counts()\n",
        "df['actor1'] = df['actor1'].apply(lambda x: 'other' if counts[x] < 100 else x)\n",
        "\n",
        "# filter actor2s having count less than 124 to \"other\"\n",
        "counts = df['actor2'].value_counts()\n",
        "df['actor2'] = df['actor2'].apply(lambda x: 'other' if counts[x] < 100 else x)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLGmf_dFm8vY",
        "source_hash": null,
        "deepnote_to_be_reexecuted": true,
        "cell_id": "7b7c161370094f8fb35c47b54e0d885c",
        "deepnote_cell_type": "code"
      },
      "source": [
        "# # This code crashes the memory\n",
        "# import pandas as pd\n",
        "# import networkx as nx\n",
        "# from sklearn.cluster import KMeans\n",
        "# import matplotlib.pyplot as plt\n",
        "# from tqdm import tqdm  # Import tqdm for the progress bar\n",
        "\n",
        "# # Load the dataset\n",
        "# data = df\n",
        "\n",
        "# # Preprocess the data\n",
        "# data['fatality_rate'] = data['fatalities'] / data['total_events']  # Calculate fatality rate\n",
        "# data = data[['event_type', 'actor1', 'fatality_rate']]  # Keep only relevant columns\n",
        "\n",
        "# # Create an empty graph\n",
        "# graph = nx.Graph()\n",
        "\n",
        "# # Add nodes to the graph\n",
        "# for i in range(data.shape[0]):\n",
        "#     graph.add_node(i)\n",
        "\n",
        "# # Add edges to the graph based on fatality rate with progress bar\n",
        "# total_edges = (data.shape[0] * (data.shape[0] - 1)) // 2  # Total number of edges to be added\n",
        "# with tqdm(total=total_edges, desc='Adding Edges') as pbar:\n",
        "#     for i in range(data.shape[0]):\n",
        "#         for j in range(i + 1, data.shape[0]):\n",
        "#             fatality_rate_i = data.loc[i, 'fatality_rate']\n",
        "#             fatality_rate_j = data.loc[j, 'fatality_rate']\n",
        "#             weight = abs(fatality_rate_i - fatality_rate_j)  # Calculate edge weight based on fatality rate difference\n",
        "#             graph.add_edge(i, j, weight=weight)\n",
        "#             pbar.update(1)  # Update the progress bar\n",
        "\n",
        "# # Perform clustering on the graph nodes\n",
        "# kmeans = KMeans(n_clusters=4)\n",
        "# kmeans.fit(data)\n",
        "\n",
        "# # Get the cluster labels\n",
        "# labels = kmeans.labels_\n",
        "\n",
        "# # Visualize the clusters using a bar chart\n",
        "# for i in range(len(set(labels))):\n",
        "#     cluster_nodes = [node for node, label in zip(graph.nodes(), labels) if label == i]\n",
        "#     event_types = data.loc[cluster_nodes, 'event_type'].value_counts()\n",
        "#     actor1s = data.loc[cluster_nodes, 'actor1'].value_counts()\n",
        "#     fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(20, 8))\n",
        "#     ax[0].bar(event_types.index, event_types.values)\n",
        "#     ax[0].set_title(f'Cluster {i}: Conflict Types')\n",
        "#     ax[0].set_xlabel('Count')\n",
        "#     ax[0].set_ylabel('Conflict Type')\n",
        "#     ax[1].bar(actor1s.index, actor1s.values)\n",
        "#     ax[1].set_title(f'Cluster {i}: Actors Involved')\n",
        "#     ax[1].set_xlabel('Actor')\n",
        "#     ax[1].set_ylabel('Count')\n",
        "#     ax[1].set_xticklabels(actor1s.index, rotation=90)\n",
        "#     plt.subplots_adjust(hspace=0.5)\n",
        "#     plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Qo3VHsQm1NI",
        "source_hash": null,
        "deepnote_to_be_reexecuted": true,
        "cell_id": "6e0924e5285640228bd78125e2cdbc0f",
        "deepnote_cell_type": "code"
      },
      "source": [
        "!pip install nmslib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Emstn522nTk0",
        "source_hash": null,
        "deepnote_to_be_reexecuted": true,
        "cell_id": "8dcb128f0b5e4a199f27a34abd9770bf",
        "deepnote_cell_type": "code"
      },
      "source": [
        "# approximate nearest neighbor search algorithms like k-d trees or locality-\n",
        "# sensitive hashing (LSH) to speed up the computation."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "353omMPtf63p",
        "source_hash": null,
        "deepnote_to_be_reexecuted": true,
        "cell_id": "79b378ff5e124c839a836c08e05faa3b",
        "deepnote_cell_type": "code"
      },
      "source": [
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import nmslib\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load the dataset\n",
        "data = df\n",
        "\n",
        "# Create an empty graph\n",
        "graph = nx.Graph()\n",
        "\n",
        "# Create an index for approximate nearest neighbor search\n",
        "index = nmslib.init(method='hnsw', space='l2')\n",
        "index.addDataPointBatch(data[['fatalities']].values)\n",
        "index.createIndex()\n",
        "\n",
        "# Add nodes to the graph\n",
        "for i in range(data.shape[0]):\n",
        "    graph.add_node(i)\n",
        "\n",
        "# Add edges to the graph based on approximate nearest neighbors and fatality rate with progress bar\n",
        "k = 10  # Number of nearest neighbors to consider\n",
        "total_edges = data.shape[0] * k  # Total number of edges to be added\n",
        "with tqdm(total=total_edges, desc='Adding Edges') as pbar:\n",
        "    neighbors = index.knnQueryBatch(data[['fatalities']].values, k=k+1)  # +1 to exclude self\n",
        "    for i, (indices, _) in enumerate(neighbors):\n",
        "        for j in indices[1:]:\n",
        "            fatality_rate_i = data.loc[i, 'fatalities']\n",
        "            fatality_rate_j = data.loc[j, 'fatalities']\n",
        "            weight = abs(fatality_rate_i - fatality_rate_j)  # Calculate edge weight based on fatality rate difference\n",
        "            graph.add_edge(i, j, weight=weight)\n",
        "            pbar.update(1)  # Update the progress bar\n",
        "\n",
        "# Perform clustering on the graph nodes\n",
        "event_types_encoded = pd.get_dummies(data['event_type'], prefix='event_type')\n",
        "kmeans = KMeans(n_clusters=3)\n",
        "kmeans.fit(event_types_encoded)\n",
        "\n",
        "# Get the cluster labels\n",
        "labels = kmeans.labels_\n",
        "\n",
        "# Visualize the clusters using a bar chart\n",
        "for i in range(len(set(labels))):\n",
        "    cluster_nodes = [node for node, label in zip(graph.nodes(), labels) if label == i]\n",
        "    event_types = data.loc[cluster_nodes, 'event_type'].value_counts()\n",
        "    plt.bar(event_types.index, event_types.values)\n",
        "    plt.title(f'Cluster {i}: Event Types')\n",
        "    plt.xlabel('Event Type')\n",
        "    plt.ylabel('Count')\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQClkC3npDu8",
        "source_hash": null,
        "deepnote_to_be_reexecuted": true,
        "cell_id": "964926f3ce4b40a4815ae7907daf6b85",
        "deepnote_cell_type": "code"
      },
      "source": [
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import nmslib\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load the dataset\n",
        "data = df\n",
        "\n",
        "# Create an empty graph\n",
        "graph = nx.Graph()\n",
        "\n",
        "# Create an index for approximate nearest neighbor search\n",
        "index = nmslib.init(method='hnsw', space='l2')\n",
        "index.addDataPointBatch(data[['fatalities']].values)\n",
        "index.createIndex()\n",
        "\n",
        "# Add nodes to the graph\n",
        "for i in range(data.shape[0]):\n",
        "    graph.add_node(i)\n",
        "\n",
        "# Add edges to the graph based on approximate nearest neighbors and fatality rate with progress bar\n",
        "k = 10  # Number of nearest neighbors to consider\n",
        "total_edges = data.shape[0] * k  # Total number of edges to be added\n",
        "with tqdm(total=total_edges, desc='Adding Edges') as pbar:\n",
        "    neighbors = index.knnQueryBatch(data[['fatalities']].values, k=k+1)  # +1 to exclude self\n",
        "    for i, (indices, _) in enumerate(neighbors):\n",
        "        for j in indices[1:]:\n",
        "            fatality_rate_i = data.loc[i, 'fatalities']\n",
        "            fatality_rate_j = data.loc[j, 'fatalities']\n",
        "            weight = abs(fatality_rate_i - fatality_rate_j)  # Calculate edge weight based on fatality rate difference\n",
        "            graph.add_edge(i, j, weight=weight)\n",
        "            pbar.update(1)  # Update the progress bar\n",
        "\n",
        "# Perform clustering on the graph nodes\n",
        "event_types_encoded = pd.get_dummies(data['event_type'], prefix='event_type')\n",
        "kmeans = KMeans(n_clusters=4)\n",
        "kmeans.fit(event_types_encoded)\n",
        "\n",
        "# Get the cluster labels\n",
        "labels = kmeans.labels_\n",
        "\n",
        "# Get the actors involved in each cluster\n",
        "actors_by_cluster = {}\n",
        "for i in range(len(set(labels))):\n",
        "    cluster_nodes = [node for node, label in zip(graph.nodes(), labels) if label == i]\n",
        "    actors = data.loc[cluster_nodes, 'actor1'].unique()\n",
        "    actors_by_cluster[i] = actors\n",
        "\n",
        "# Visualize the clusters and actors using a bar chart\n",
        "for cluster, actors in actors_by_cluster.items():\n",
        "    actor_counts = data[data['actor1'].isin(actors)]['actor1'].value_counts()\n",
        "    plt.bar(actor_counts.index, actor_counts.values)\n",
        "    plt.title(f'Cluster {cluster}: Actors Involved')\n",
        "    plt.xlabel('Actor')\n",
        "    plt.ylabel('Count')\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBYzHBwjpfwV",
        "source_hash": null,
        "deepnote_to_be_reexecuted": true,
        "cell_id": "4702e827830446feb3b9ebe9811a88e2",
        "deepnote_cell_type": "code"
      },
      "source": [
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import nmslib\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load the dataset\n",
        "data = df\n",
        "\n",
        "# Create an empty graph\n",
        "graph = nx.Graph()\n",
        "\n",
        "# Create an index for approximate nearest neighbor search\n",
        "index = nmslib.init(method='hnsw', space='l2')\n",
        "index.addDataPointBatch(data[['fatalities']].values)\n",
        "index.createIndex()\n",
        "\n",
        "# Add nodes to the graph\n",
        "for i in range(data.shape[0]):\n",
        "    graph.add_node(i)\n",
        "\n",
        "# Add edges to the graph based on approximate nearest neighbors and fatality rate with progress bar\n",
        "k = 10  # Number of nearest neighbors to consider\n",
        "total_edges = data.shape[0] * k  # Total number of edges to be added\n",
        "with tqdm(total=total_edges, desc='Adding Edges') as pbar:\n",
        "    neighbors = index.knnQueryBatch(data[['fatalities']].values, k=k+1)  # +1 to exclude self\n",
        "    for i, (indices, _) in enumerate(neighbors):\n",
        "        for j in indices[1:]:\n",
        "            fatality_rate_i = data.loc[i, 'fatalities']\n",
        "            fatality_rate_j = data.loc[j, 'fatalities']\n",
        "            weight = abs(fatality_rate_i - fatality_rate_j)  # Calculate edge weight based on fatality rate difference\n",
        "            graph.add_edge(i, j, weight=weight)\n",
        "            pbar.update(1)  # Update the progress bar\n",
        "\n",
        "# Perform clustering on the graph nodes\n",
        "event_types_encoded = pd.get_dummies(data['event_type'], prefix='event_type')\n",
        "kmeans = KMeans(n_clusters=3)\n",
        "kmeans.fit(event_types_encoded)\n",
        "\n",
        "# Get the cluster labels\n",
        "labels = kmeans.labels_\n",
        "\n",
        "# Get the actors involved in each cluster\n",
        "actors_by_cluster = {}\n",
        "for i in range(len(set(labels))):\n",
        "    cluster_nodes = [node for node, label in zip(graph.nodes(), labels) if label == i]\n",
        "    actors = data.loc[cluster_nodes, 'actor1'].unique()\n",
        "    actors_by_cluster[i] = actors\n",
        "\n",
        "# Visualize the clusters, event types, and actors using a bar chart\n",
        "fig, ax = plt.subplots(nrows=len(set(labels)), ncols=2, figsize=(20, 50))\n",
        "for i, (cluster, actors) in enumerate(actors_by_cluster.items()):\n",
        "    cluster_nodes = [node for node, label in zip(graph.nodes(), labels) if label == cluster]\n",
        "    event_types = data.loc[cluster_nodes, 'event_type'].value_counts()\n",
        "    actor_counts = data[data['actor1'].isin(actors)]['actor1'].value_counts()\n",
        "\n",
        "    ax[i, 0].bar(event_types.index, event_types.values)\n",
        "    ax[i, 0].set_title(f'Cluster {cluster}: Event Types \\n')\n",
        "    ax[i, 0].set_xlabel('Event Type')\n",
        "    ax[i, 0].set_ylabel('Count')\n",
        "    ax[i, 0].tick_params(axis='x', rotation=90)\n",
        "\n",
        "    ax[i, 1].bar(actor_counts.index, actor_counts.values)\n",
        "    ax[i, 1].set_title(f'Cluster {cluster}: Actors Involved')\n",
        "    ax[i, 1].set_xlabel('Actor')\n",
        "    ax[i, 1].set_ylabel('Count')\n",
        "    ax[i, 1].tick_params(axis='x', rotation=90)\n",
        "\n",
        "plt.subplots_adjust(hspace=0.5)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMJq9cNMtBRJ",
        "cell_id": "6f3dbdd0d54b4e7a8bd727430829c8c8",
        "deepnote_cell_type": "markdown"
      },
      "source": [
        "# Month with more fatality of every years and which actors, locations and event was there?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1oZ3eP-uahN",
        "source_hash": null,
        "deepnote_to_be_reexecuted": true,
        "cell_id": "0190d392f00648149deca62dd7cd53dd",
        "deepnote_cell_type": "code"
      },
      "source": [
        "df.sample(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2CpDwgaucv7",
        "source_hash": null,
        "deepnote_to_be_reexecuted": true,
        "cell_id": "6c7f747963694290883c76ff31753e29",
        "deepnote_cell_type": "code"
      },
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load data\n",
        "data = df\n",
        "\n",
        "# Calculate the average fatalities by year and month\n",
        "data_avg = data.groupby(['year', 'month']).agg({'fatalities': 'mean'}).reset_index()\n",
        "\n",
        "# Pivot the data to create a heatmap\n",
        "data_pivot = data_avg.pivot_table(values='fatalities', index='year', columns='month')\n",
        "\n",
        "# Find the months with the highest average fatality\n",
        "highest_months = data_pivot.mean().idxmax()\n",
        "\n",
        "# Filter the data for the highest months\n",
        "highest_data = data[(data['year'].isin(data_pivot.index)) & (data['month'] == highest_months)]\n",
        "\n",
        "# Analyze the actors involved\n",
        "actors_involved = highest_data[['actor1', 'actor2']].stack().value_counts()\n",
        "\n",
        "# Analyze the locations\n",
        "locations = highest_data['admin1'].value_counts()\n",
        "\n",
        "# Create a heatmap using seaborn\n",
        "sns.heatmap(data_pivot, cmap='Reds')\n",
        "\n",
        "# Set plot title and axis labels\n",
        "plt.title('Monthly Fatalities by Year')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Year')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n",
        "\n",
        "# Print the analysis results\n",
        "print(f\"\\nMonths with Highest Average Fatality: {highest_months}\")\n",
        "print(\"\\nActors Involved:\")\n",
        "print(actors_involved)\n",
        "print(\"\\nLocations:\")\n",
        "print(locations)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axtAieHxvoVU",
        "source_hash": null,
        "deepnote_to_be_reexecuted": true,
        "cell_id": "db72c4ee0a8b4b04bf4d51cdce926697",
        "deepnote_cell_type": "code"
      },
      "source": [
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load data\n",
        "data = df\n",
        "\n",
        "# Calculate the average fatalities by year and month\n",
        "data_avg = data.groupby(['year', 'month']).agg({'fatalities': 'mean'}).reset_index()\n",
        "\n",
        "# Find the month with the highest average fatality\n",
        "highest_month = data_avg.loc[data_avg['fatalities'].idxmax(), 'month']\n",
        "\n",
        "# Filter the data for the highest fatality month\n",
        "highest_data = data[(data['month'] == highest_month)]\n",
        "\n",
        "# Get the top 5 actors and locations with the highest counts\n",
        "top_actors = highest_data['actor1'].value_counts().nlargest(5).index.tolist()\n",
        "top_locations = highest_data['admin1'].value_counts().nlargest(5).index.tolist()\n",
        "\n",
        "# Create a graph\n",
        "graph = nx.Graph()\n",
        "\n",
        "# Add nodes for actors and locations\n",
        "graph.add_nodes_from(top_actors, bipartite='actors')\n",
        "graph.add_nodes_from(top_locations, bipartite='locations')\n",
        "\n",
        "# Add edges for actor-month and location-month relationships\n",
        "actor_month_counts = highest_data.groupby(['actor1', 'month']).size().reset_index(name='count')\n",
        "location_month_counts = highest_data.groupby(['admin1', 'month']).size().reset_index(name='count')\n",
        "\n",
        "for _, row in actor_month_counts.iterrows():\n",
        "    if row['actor1'] in top_actors:\n",
        "        graph.add_edge(row['actor1'], row['month'], weight=row['count'])\n",
        "\n",
        "for _, row in location_month_counts.iterrows():\n",
        "    if row['admin1'] in top_locations:\n",
        "        graph.add_edge(row['admin1'], row['month'], weight=row['count'])\n",
        "\n",
        "# Set the layout for the graph\n",
        "pos = nx.spring_layout(graph, seed=42)\n",
        "\n",
        "# Plot the graph\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Draw actor-month edges\n",
        "actor_edges = [(actor, month) for actor, month in graph.edges() if graph.nodes[actor]['bipartite'] == 'actors']\n",
        "actor_weights = [graph.edges[actor, month]['weight'] for actor, month in actor_edges]\n",
        "nx.draw_networkx_edges(graph, pos, edgelist=actor_edges,  alpha=0.6, edge_color='blue') #width=actor_weights,\n",
        "\n",
        "# Draw location-month edges\n",
        "location_edges = [(location, month) for location, month in graph.edges() if graph.nodes[location]['bipartite'] == 'locations']\n",
        "location_weights = [graph.edges[location, month]['weight'] for location, month in location_edges]\n",
        "nx.draw_networkx_edges(graph, pos, edgelist=location_edges,  alpha=0.6, edge_color='red') #width=location_weights,\n",
        "\n",
        "# Draw actor and location nodes\n",
        "nx.draw_networkx_nodes(graph, pos, nodelist=top_actors, node_color='blue', node_size=200)\n",
        "nx.draw_networkx_nodes(graph, pos, nodelist=top_locations, node_color='red', node_size=200)\n",
        "\n",
        "# Draw month nodes\n",
        "nx.draw_networkx_nodes(graph, pos, nodelist=[highest_month], node_color='green', node_size=300)\n",
        "\n",
        "# Draw labels\n",
        "nx.draw_networkx_labels(graph, pos, font_size=8, font_color='black')\n",
        "\n",
        "# Set plot title and axis labels\n",
        "plt.title(f'Highest Fatality Month: {highest_month}')\n",
        "plt.xlabel('Actors / Locations')\n",
        "plt.ylabel('Month')\n",
        "\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDY19uzv0qhf",
        "source_hash": null,
        "deepnote_to_be_reexecuted": true,
        "cell_id": "124ccf392aec42038912241c39666330",
        "deepnote_cell_type": "code"
      },
      "source": [
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load data\n",
        "data = df\n",
        "\n",
        "# Calculate the average fatalities by year and month\n",
        "data_avg = data.groupby(['year', 'month']).agg({'fatalities': 'mean'}).reset_index()\n",
        "\n",
        "# Find the month with the highest average fatality\n",
        "highest_month = data_avg.loc[data_avg['fatalities'].idxmax(), 'month']\n",
        "\n",
        "# Filter the data for the highest fatality month\n",
        "highest_data = data[(data['month'] == highest_month)]\n",
        "\n",
        "# Get the top 5 actors and locations with the highest counts\n",
        "top_actors = highest_data['actor1'].value_counts().nlargest(5).index.tolist()\n",
        "top_locations = highest_data['admin1'].value_counts().nlargest(5).index.tolist()\n",
        "\n",
        "# Create a graph\n",
        "graph = nx.Graph()\n",
        "\n",
        "# Add nodes for actors and locations\n",
        "graph.add_nodes_from(top_actors, bipartite='actors')\n",
        "graph.add_nodes_from(top_locations, bipartite='locations')\n",
        "\n",
        "# Add edges for actor-month and location-month relationships\n",
        "actor_month_counts = highest_data.groupby(['actor1', 'month']).size().reset_index(name='count')\n",
        "location_month_counts = highest_data.groupby(['admin1', 'month']).size().reset_index(name='count')\n",
        "\n",
        "for _, row in actor_month_counts.iterrows():\n",
        "    if row['actor1'] in top_actors:\n",
        "        graph.add_edge(row['actor1'], row['month'], count=row['count'])\n",
        "\n",
        "for _, row in location_month_counts.iterrows():\n",
        "    if row['admin1'] in top_locations:\n",
        "        graph.add_edge(row['admin1'], row['month'], count=row['count'])\n",
        "\n",
        "# Set the layout for the graph\n",
        "pos = nx.spring_layout(graph, seed=42)\n",
        "\n",
        "# Plot the graph\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Draw actor-month edges\n",
        "actor_edges = [(actor, month) for actor, month, data in graph.edges(data=True) if graph.nodes[actor]['bipartite'] == 'actors']\n",
        "nx.draw_networkx_edges(graph, pos, edgelist=actor_edges, width=1.0, alpha=0.6, edge_color='blue')\n",
        "\n",
        "# Draw location-month edges\n",
        "location_edges = [(location, month) for location, month, data in graph.edges(data=True) if graph.nodes[location]['bipartite'] == 'locations']\n",
        "nx.draw_networkx_edges(graph, pos, edgelist=location_edges, width=1.0, alpha=0.6, edge_color='red')\n",
        "\n",
        "# Draw actor and location nodes\n",
        "nx.draw_networkx_nodes(graph, pos, nodelist=top_actors, node_color='blue', node_size=200)\n",
        "nx.draw_networkx_nodes(graph, pos, nodelist=top_locations, node_color='red', node_size=200)\n",
        "\n",
        "# Draw month nodes\n",
        "nx.draw_networkx_nodes(graph, pos, nodelist=[highest_month], node_color='green', node_size=300)\n",
        "\n",
        "# Draw labels\n",
        "nx.draw_networkx_labels(graph, pos, font_size=8, font_color='black')\n",
        "\n",
        "# Draw edge labels\n",
        "edge_labels = {(u, v): data['count'] for u, v, data in graph.edges(data=True)}\n",
        "nx.draw_networkx_edge_labels(graph, pos, edge_labels=edge_labels, font_size=6)\n",
        "\n",
        "# Set plot title and axis labels\n",
        "plt.title(f'Highest Fatality Month: {highest_month}')\n",
        "plt.xlabel('Actors / Locations')\n",
        "plt.ylabel('Month')\n",
        "\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhoMhmOW4R0x",
        "source_hash": null,
        "deepnote_to_be_reexecuted": true,
        "cell_id": "1b2f7a02af704422ba2d43cfb8ecabd0",
        "deepnote_cell_type": "code"
      },
      "source": [
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load data\n",
        "data = df\n",
        "\n",
        "# Calculate the average fatalities by year and month\n",
        "data_avg = data.groupby(['year', 'month']).agg({'fatalities': 'mean'}).reset_index()\n",
        "\n",
        "# Find the months with the highest and second highest average fatality\n",
        "highest_month = data_avg.loc[data_avg['fatalities'].idxmax(), 'month']\n",
        "data_avg = data_avg.drop(data_avg['fatalities'].idxmax())\n",
        "second_highest_month = data_avg.loc[data_avg['fatalities'].idxmax(), 'month']\n",
        "\n",
        "# Filter the data for the highest fatality month\n",
        "highest_data = data[(data['month'] == highest_month)]\n",
        "\n",
        "# Get the top 5 actors and locations with the highest counts for the highest month\n",
        "top_actors = highest_data['actor1'].value_counts().nlargest(5).index.tolist()\n",
        "top_locations = highest_data['admin1'].value_counts().nlargest(5).index.tolist()\n",
        "\n",
        "# Create a graph\n",
        "graph = nx.Graph()\n",
        "\n",
        "# Add nodes for actors and locations\n",
        "graph.add_nodes_from(top_actors, bipartite='actors')\n",
        "graph.add_nodes_from(top_locations, bipartite='locations')\n",
        "\n",
        "# Add edges for actor-month and location-month relationships for the highest month\n",
        "actor_month_counts = highest_data.groupby(['actor1', 'month']).size().reset_index(name='count')\n",
        "location_month_counts = highest_data.groupby(['admin1', 'month']).size().reset_index(name='count')\n",
        "\n",
        "for _, row in actor_month_counts.iterrows():\n",
        "    if row['actor1'] in top_actors:\n",
        "        graph.add_edge(row['actor1'], row['month'], count=row['count'])\n",
        "\n",
        "for _, row in location_month_counts.iterrows():\n",
        "    if row['admin1'] in top_locations:\n",
        "        graph.add_edge(row['admin1'], row['month'], count=row['count'])\n",
        "\n",
        "# Filter the data for the second highest fatality month\n",
        "second_highest_data = data[(data['month'] == second_highest_month)]\n",
        "\n",
        "# Get the top 5 actors and locations with the highest counts for the second highest month\n",
        "second_top_actors = second_highest_data['actor1'].value_counts().nlargest(5).index.tolist()\n",
        "second_top_locations = second_highest_data['admin1'].value_counts().nlargest(5).index.tolist()\n",
        "\n",
        "# Add nodes for actors and locations for the second highest month\n",
        "graph.add_nodes_from(second_top_actors, bipartite='actors')\n",
        "graph.add_nodes_from(second_top_locations, bipartite='locations')\n",
        "\n",
        "# Add edges for actor-month and location-month relationships for the second highest month\n",
        "actor_month_counts = second_highest_data.groupby(['actor1', 'month']).size().reset_index(name='count')\n",
        "location_month_counts = second_highest_data.groupby(['admin1', 'month']).size().reset_index(name='count')\n",
        "\n",
        "for _, row in actor_month_counts.iterrows():\n",
        "    if row['actor1'] in second_top_actors:\n",
        "        graph.add_edge(row['actor1'], row['month'], count=row['count'])\n",
        "\n",
        "for _, row in location_month_counts.iterrows():\n",
        "    if row['admin1'] in second_top_locations:\n",
        "        graph.add_edge(row['admin1'], row['month'], count=row['count'])\n",
        "\n",
        "# Set the layout for the graph\n",
        "pos = nx.spring_layout(graph, seed=42)\n",
        "\n",
        "# Plot the graph\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Draw actor-month edges for the highest month\n",
        "actor_edges = [(actor, month) for actor, month, data in graph.edges(data=True) if graph.nodes[actor]['bipartite'] == 'actors' and month == highest_month]\n",
        "actor_weights = [data['count'] for actor, month, data in graph.edges(data=True) if graph.nodes[actor]['bipartite'] == 'actors' and month == highest_month]\n",
        "nx.draw_networkx_edges(graph, pos, edgelist=actor_edges, alpha=0.6, edge_color='blue', )\n",
        "\n",
        "# Draw location-month edges for the highest month\n",
        "location_edges = [(location, month) for location, month, data in graph.edges(data=True) if graph.nodes[location]['bipartite'] == 'locations' and month == highest_month]\n",
        "location_weights = [data['count'] for location, month, data in graph.edges(data=True) if graph.nodes[location]['bipartite'] == 'locations' and month == highest_month]\n",
        "nx.draw_networkx_edges(graph, pos, edgelist=location_edges, alpha=0.6, edge_color='red', )\n",
        "\n",
        "# Draw actor-month edges for the second highest month\n",
        "actor_edges = [(actor, month) for actor, month, data in graph.edges(data=True) if graph.nodes[actor]['bipartite'] == 'actors' and month == second_highest_month]\n",
        "actor_weights = [data['count'] for actor, month, data in graph.edges(data=True) if graph.nodes[actor]['bipartite'] == 'actors' and month == second_highest_month]\n",
        "nx.draw_networkx_edges(graph, pos, edgelist=actor_edges, alpha=0.6, edge_color='blue')\n",
        "\n",
        "# Draw location-month edges for the second highest month\n",
        "location_edges = [(location, month) for location, month, data in graph.edges(data=True) if graph.nodes[location]['bipartite'] == 'locations' and month == second_highest_month]\n",
        "location_weights = [data['count'] for location, month, data in graph.edges(data=True) if graph.nodes[location]['bipartite'] == 'locations' and month == second_highest_month]\n",
        "nx.draw_networkx_edges(graph, pos, edgelist=location_edges, alpha=0.6, edge_color='red')\n",
        "\n",
        "# Draw actor and location nodes\n",
        "nx.draw_networkx_nodes(graph, pos, nodelist=top_actors, node_color='blue', node_size=200)\n",
        "nx.draw_networkx_nodes(graph, pos, nodelist=top_locations, node_color='red', node_size=200)\n",
        "nx.draw_networkx_nodes(graph, pos, nodelist=second_top_actors, node_color='blue', node_size=200)\n",
        "nx.draw_networkx_nodes(graph, pos, nodelist=second_top_locations, node_color='red', node_size=200)\n",
        "\n",
        "# Draw month nodes\n",
        "nx.draw_networkx_nodes(graph, pos, nodelist=[highest_month], node_color='green', node_size=300)\n",
        "nx.draw_networkx_nodes(graph, pos, nodelist=[second_highest_month], node_color='yellow', node_size=300)\n",
        "\n",
        "# Draw labels\n",
        "nx.draw_networkx_labels(graph, pos, font_size=8, font_color='black')\n",
        "\n",
        "# Set plot title and axis labels\n",
        "plt.title(f'Highest Fatality Month: {highest_month} | Second Highest Fatality Month: {second_highest_month}')\n",
        "plt.xlabel('Actors / Locations')\n",
        "plt.ylabel('Month')\n",
        "\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpc8T2CQ6_mZ",
        "cell_id": "daf1e7c821f74079b89cefeeec29b0e8",
        "deepnote_cell_type": "markdown"
      },
      "source": [
        "## As we already finds what to expect when actors involved together. we can use this type of graph to see on which location which actors are involved in serious conflicts and what type of conflict is it. it will help law enforcement agencies to take preventive measurement accordingly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dB6Nr47v7a-_",
        "cell_id": "121764ddca204f058272ddd6dbc2681c",
        "deepnote_cell_type": "markdown"
      },
      "source": [
        "# GNN and Predicting Fatality"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHYaxBJs_qxz",
        "source_hash": null,
        "deepnote_to_be_reexecuted": true,
        "cell_id": "e44e1bf82e1540e1b4603afc38d3792e",
        "deepnote_cell_type": "code"
      },
      "source": [
        "!pip install torch_geometric"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WY9SyK4AvpG",
        "source_hash": null,
        "deepnote_to_be_reexecuted": true,
        "cell_id": "7b996881cf604fb5ac2283c84cdcc661",
        "deepnote_cell_type": "code"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Datasets/Armed_conflict_2001-01-01-2021-11-01-South_Asia-Bangladesh.csv')\n",
        "\n",
        "remove_column=['data_id','iso','event_id_cnty','event_id_no_cnty','time_precision','notes','timestamp',\n",
        "               'iso3','region','country','geo_precision','source','source_scale','notes','interaction',\n",
        "               'inter1','inter2'\n",
        "               ]\n",
        "df.drop(columns=remove_column,inplace=True)\n",
        "\n",
        "from datetime import datetime\n",
        "month=[]\n",
        "\n",
        "for date_str in df['event_date']:\n",
        "  date_object = datetime.strptime(date_str, '%d %B %Y').date()\n",
        "  month.append(date_object.month)\n",
        "\n",
        "df['month']=month\n",
        "\n",
        "\n",
        "df.drop(columns=['event_date'],inplace=True)\n",
        "my_column = df.pop('month')\n",
        "df.insert(3, 'month', my_column)\n",
        "my_column = df.pop('month')\n",
        "df.insert(1, 'month', my_column)\n",
        "\n",
        "df = df.replace(np.nan, \"unknown\")\n",
        "\n",
        "# filter actor1s having count less than 124 to \"other\"\n",
        "counts = df['actor1'].value_counts()\n",
        "df['actor1'] = df['actor1'].apply(lambda x: 'other' if counts[x] < 193 else x)\n",
        "\n",
        "# filter actor2s having count less than 124 to \"other\"\n",
        "counts = df['actor2'].value_counts()\n",
        "df['actor2'] = df['actor2'].apply(lambda x: 'other' if counts[x] < 193 else x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKsLj83C_fdx",
        "source_hash": null,
        "deepnote_to_be_reexecuted": true,
        "cell_id": "4a36d7883a0744ce88c4ed93367ce2a0",
        "deepnote_cell_type": "code"
      },
      "source": [
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GCNConv\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Load the data\n",
        "data = df\n",
        "\n",
        "# Define the nodes and edges\n",
        "event_nodes = data.index.astype(str).tolist()  # Convert event nodes to strings\n",
        "actor_nodes = pd.unique(data[['assoc_actor_1', 'assoc_actor_2']].values.ravel('K')).astype(str).tolist()  # Convert actor nodes to strings\n",
        "edges = data[['assoc_actor_1', 'assoc_actor_2']].dropna().astype(str).values  # Convert edges to strings\n",
        "\n",
        "# Create a graph\n",
        "G = nx.Graph()\n",
        "G.add_nodes_from(event_nodes, type='event')\n",
        "G.add_nodes_from(actor_nodes, type='actor')\n",
        "G.add_edges_from(edges)\n",
        "\n",
        "# Assign node and edge features\n",
        "node_features = pd.get_dummies(data['event_type']).values.astype(float)\n",
        "edge_features = None  # You can assign features to edges if required\n",
        "\n",
        "# Convert the graph to PyTorch Geometric data format\n",
        "x = torch.tensor(node_features, dtype=torch.float)\n",
        "y = torch.tensor(data['fatalities'].values, dtype=torch.float)\n",
        "\n",
        "node_index = {node: index for index, node in enumerate(G.nodes())}\n",
        "edge_index = []\n",
        "for edge in G.edges():\n",
        "    src, tgt = edge\n",
        "    src_idx = node_index[src]\n",
        "    tgt_idx = node_index[tgt]\n",
        "    edge_index.append([src_idx, tgt_idx])\n",
        "    edge_index.append([tgt_idx, src_idx])\n",
        "edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
        "\n",
        "data = Data(x=x, edge_index=edge_index, y=y)\n",
        "\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "# Shuffle the indices of the data\n",
        "indices = torch.randperm(len(data))\n",
        "\n",
        "# Define the split ratio\n",
        "split_ratio = 0.8  # 80% for training, 20% for testing\n",
        "\n",
        "# Determine the split index\n",
        "split_idx = int(split_ratio * len(data))\n",
        "\n",
        "# Split the indices into training and testing sets\n",
        "train_indices = indices[:split_idx]\n",
        "test_indices = indices[split_idx:]\n",
        "\n",
        "# Create the training and testing data objects\n",
        "data_train = data[train_indices]\n",
        "data_test = data[test_indices]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define the GNN model\n",
        "class GNNModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(GNNModel, self).__init__()\n",
        "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = torch.relu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "# Initialize the GNN model\n",
        "input_dim = node_features.shape[1]\n",
        "hidden_dim = 64\n",
        "output_dim = 1  # Regression task\n",
        "model = GNNModel(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 100\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data_train.x, data_train.edge_index)\n",
        "    loss = criterion(out[data_train.train_mask], data_train.y[data_train.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    out = model(data_test.x, data_test.edge_index)\n",
        "    y_pred = out[data_test.test_mask].detach().numpy()\n",
        "    y_true = data_test.y[data_test.test_mask].numpy()\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    print(f\"Mean Squared Error: {mse}\")\n",
        "    print(f\"R-squared: {r2}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1tg7guaK816",
        "source_hash": null,
        "deepnote_to_be_reexecuted": true,
        "cell_id": "1664de803a33412b9e190641b0cb0829",
        "deepnote_cell_type": "code"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XO-FWkg_-hz",
        "source_hash": null,
        "deepnote_to_be_reexecuted": true,
        "cell_id": "2e7c8f9b4cc943d7aa792664946a69b0",
        "deepnote_cell_type": "code"
      },
      "source": [
        "# Convert the graph to TensorFlow data format\n",
        "adjacency_matrix = nx.adjacency_matrix(G).toarray()\n",
        "adjacency_matrix = adjacency_matrix[:len(event_nodes), :len(event_nodes)]  # Trim adjacency matrix to match node_features\n",
        "a = tf.constant(adjacency_matrix, dtype=tf.float32)\n",
        "\n",
        "# Verify the shapes\n",
        "print(f\"Shape of x: {x.shape}\")\n",
        "print(f\"Shape of y: {y.shape}\")\n",
        "print(f\"Shape of a: {a.shape}\")\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "x_train, x_test, y_train, y_test, a_train, a_test = train_test_split(x.numpy(), y.numpy(), a.numpy(), test_size=0.2)\n",
        "\n",
        "class GraphConvolution(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(GraphConvolution, self).__init__()\n",
        "        self.units = units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        input_dim = input_shape[-1]\n",
        "        self.kernel = self.add_weight(\"kernel\", shape=(input_dim, self.units))\n",
        "\n",
        "    def call(self, inputs, adjacency):\n",
        "        output = tf.matmul(adjacency, tf.matmul(inputs, self.kernel))\n",
        "        return output\n",
        "\n",
        "class GNNModel(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(GNNModel, self).__init__()\n",
        "        self.conv1 = GraphConvolution(units=64)\n",
        "        self.conv2 = GraphConvolution(units=1)\n",
        "\n",
        "    def call(self, inputs, adjacency):\n",
        "        x = self.conv1(inputs, adjacency)\n",
        "        x = tf.nn.relu(x)\n",
        "        x = self.conv2(x, adjacency)\n",
        "        return x\n",
        "\n",
        "# Convert the graph to TensorFlow data format\n",
        "adjacency_matrix = nx.adjacency_matrix(G).toarray()\n",
        "adjacency_matrix = adjacency_matrix[:len(event_nodes), :len(event_nodes)]  # Trim adjacency matrix to match node_features\n",
        "a = tf.constant(adjacency_matrix, dtype=tf.float32)\n",
        "\n",
        "# Verify the shapes\n",
        "print(f\"Shape of x: {x.shape}\")\n",
        "print(f\"Shape of y: {y.shape}\")\n",
        "print(f\"Shape of a: {a.shape}\")\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "x_train, x_test, y_train, y_test, a_train, a_test = train_test_split(x.numpy(), y.numpy(), a.numpy(), test_size=0.2)\n",
        "\n",
        "# Initialize the GNN model\n",
        "model = GNNModel()\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "loss_object = tf.keras.losses.MeanSquaredError()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "\n",
        "# Training\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(x_train, a_train)\n",
        "        loss = loss_object(y_train, predictions)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "# Evaluation\n",
        "predictions = model(x_test, a_test)\n",
        "mse = mean_squared_error(y_test, predictions.numpy())\n",
        "r2 = r2_score(y_test, predictions.numpy())\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "print(f\"R-squared: {r2}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdaGI4Y8_sNB",
        "source_hash": null,
        "deepnote_to_be_reexecuted": true,
        "cell_id": "e18f0e5395a7403bb008ade85a4fe63a",
        "deepnote_cell_type": "code"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "f7ed51584a9e4a63a6656f98ea2f810a",
        "deepnote_cell_type": "visualization",
        "id": "pAArhYGorThh"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=b3a63533-b289-453b-964c-81d5b691aecc' target=\"_blank\">\n",
        "<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n",
        "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
      ],
      "metadata": {
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown",
        "id": "KsjjDAqmrThh"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "deepnote": {},
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "deepnote_notebook_id": "85a6272e8fef49f7ad15bf4aca216ee4",
    "deepnote_execution_queue": []
  }
}