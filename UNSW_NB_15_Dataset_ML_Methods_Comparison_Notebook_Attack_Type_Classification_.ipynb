{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vatiza/Machine_Learning/blob/main/UNSW_NB_15_Dataset_ML_Methods_Comparison_Notebook_Attack_Type_Classification_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ul-iGa4rPb81"
      },
      "source": [
        "# UNSW-NB 15 Dataset ML Methods Comparison Notebook - Attack Type Classification\n",
        "\n",
        "This notebook will implement several machine learning methods:\n",
        "\n",
        "- Logistic Regression\n",
        "- Decision Trees\n",
        "- Random Forest\n",
        "- Neural Networks: we will use Multi-layer perceptron.\n",
        "\n",
        "We will illustrate them and compare their performances the The raw network packets of the UNSW-NB 15 dataset was created by the IXIA PerfectStorm tool in the Cyber Range Lab of the Australian Centre for Cyber Security (ACCS) for generating a hybrid of real modern normal activities and synthetic contemporary attack behaviours. Further information found at https://www.unsw.adfa.edu.au/unsw-canberra-cyber/cybersecurity/ADFA-NB15-Datasets/\n",
        "\n",
        "\n",
        "\n",
        "![unsw-nb15-testbed.jpg](attachment:unsw-nb15-testbed.jpg)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Tcpdump tool is utilised to capture 100 GB of the raw traffic (e.g., Pcap files). This dataset has nine types of attacks, namely, Fuzzers, Analysis, Backdoors, DoS, Exploits, Generic, Reconnaissance, Shellcode and Worms. The Argus, Bro-IDS tools are used and twelve algorithms are developed to generate totally 49 features with the class label.\n",
        "\n",
        "These features are described in UNSW-NB15_features.csv file.\n",
        "\n",
        "A partition from this dataset is configured as a training set and testing set, namely, UNSW_NB15_training-set.csv and UNSW_NB15_testing-set.csv respectively.\n",
        "\n",
        "The number of records in the training set is 175,341 records and the testing set is 82,332 records from the different types, attack and normal.Figure 1 and 2 show the testbed configuration dataset and the method of the feature creation of the UNSW-NB15, respectively.\n",
        "\n",
        "The details of the UNSW-NB15 dataset are published in following the papers:\n",
        "\n",
        "Moustafa, Nour, and Jill Slay. \"UNSW-NB15: a comprehensive data set for network intrusion detection systems (UNSW-NB15 network data set).\" Military Communications and Information Systems Conference (MilCIS), 2015. IEEE, 2015.\n",
        "Moustafa, Nour, and Jill Slay. \"The evaluation of Network Anomaly Detection Systems: Statistical analysis of the UNSW-NB15 dataset and the comparison with the KDD99 dataset.\" Information Security Journal: A Global Perspective (2016): 1-14.\n",
        "Moustafa, Nour, et al. . \"Novel geometric area analysis technique for anomaly detection using trapezoidal area estimation on large-scale networks.\" IEEE Transactions on Big Data (2017).\n",
        "Moustafa, Nour, et al. \"Big data analytics for intrusion detection system: statistical decision-making using finite dirichlet mixture models.\" Data Analytics and Decision Support for Cybersecurity. Springer, Cham, 2017. 127-156.\n",
        "\n",
        "\n",
        "For more information, please contact the authors: Harshil Patel & Yuesheng Chen are a students in Industrial Engineering at Ohio State University, and they are interested in new Cyber threat intelligence approaches and the technology of Industry 4.0.\n",
        "\n",
        "In this notebook, the operations conducted include:\n",
        "\n",
        "- Preprocessing the data to prepare for training ML models.\n",
        "- Training ML models based on cross-validation.\n",
        "- Evaluating ML models based on testing data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3z0COl30Pb85"
      },
      "source": [
        "# Libraries\n",
        "\n",
        "Import libararies to implement the described machine learning methods using a few different `sklearn` algorithms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBUVXQdYPb85"
      },
      "outputs": [],
      "source": [
        "# data cleaning and plots\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "%matplotlib inline\n",
        "\n",
        "# sklearn: data preprocessing\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
        "\n",
        "# sklearn: train model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score, cross_validate, StratifiedKFold\n",
        "from sklearn.metrics import precision_recall_curve, precision_score, recall_score, f1_score, accuracy_score\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score, confusion_matrix, classification_report\n",
        "\n",
        "# sklearn classifiers\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vlHGvWBPb86"
      },
      "source": [
        "# Data Processing\n",
        "\n",
        "## Load Data\n",
        "\n",
        "**UNSW-NB15: a comprehensive data set for network intrusion detection systems**\n",
        "\n",
        "These features are described in UNSW-NB15_features.csv file.\n",
        "\n",
        "A partition from this dataset is configured as a training set and testing set, namely, UNSW_NB15_training-set.csv and UNSW_NB15_testing-set.csv respectively.\n",
        "\n",
        "The number of records in the training set is 175,341 records and the testing set is 82,332 records from the different types, attack and normal.Figure 1 and 2 show the testbed configuration dataset and the method of the feature creation of the UNSW-NB15, respectively. The addtional features are as described in UNSW-NB15_features.csv file.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJ3WQFlQPb87"
      },
      "source": [
        "## Response Variable:\n",
        "\n",
        "attack_cat: This dataset has nine types of attacks, namely, Fuzzers, Analysis, Backdoors, DoS, Exploits, Generic, Reconnaissance, Shellcode and Worms.\n",
        "\n",
        "Label: 0 for normal and 1 for attack records\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGXGdHf-PhEJ",
        "outputId": "9dde5912-7636-4193-c2d7-cdc89bd1503f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYzVKCV0Pb87"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "initial_data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/The UNSW-NB15 Dataset/ClassificationData/UNSW_NB15_training-set.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "PRw94_j4Pb87",
        "outputId": "778d8d32-cbae-454c-9e94-09c6525ee064"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   id       dur proto service state  spkts  dpkts  sbytes  dbytes  \\\n",
              "0   1  0.000011   udp       -   INT      2      0     496       0   \n",
              "1   2  0.000008   udp       -   INT      2      0    1762       0   \n",
              "2   3  0.000005   udp       -   INT      2      0    1068       0   \n",
              "3   4  0.000006   udp       -   INT      2      0     900       0   \n",
              "4   5  0.000010   udp       -   INT      2      0    2126       0   \n",
              "\n",
              "          rate  ...  ct_dst_sport_ltm  ct_dst_src_ltm  is_ftp_login  \\\n",
              "0   90909.0902  ...                 1               2             0   \n",
              "1  125000.0003  ...                 1               2             0   \n",
              "2  200000.0051  ...                 1               3             0   \n",
              "3  166666.6608  ...                 1               3             0   \n",
              "4  100000.0025  ...                 1               3             0   \n",
              "\n",
              "   ct_ftp_cmd  ct_flw_http_mthd  ct_src_ltm  ct_srv_dst  is_sm_ips_ports  \\\n",
              "0           0                 0           1           2                0   \n",
              "1           0                 0           1           2                0   \n",
              "2           0                 0           1           3                0   \n",
              "3           0                 0           2           3                0   \n",
              "4           0                 0           2           3                0   \n",
              "\n",
              "   attack_cat  label  \n",
              "0      Normal      0  \n",
              "1      Normal      0  \n",
              "2      Normal      0  \n",
              "3      Normal      0  \n",
              "4      Normal      0  \n",
              "\n",
              "[5 rows x 45 columns]"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-338e92cd-5a22-447d-8dc3-62e2d78c16da\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>dur</th>\n",
              "      <th>proto</th>\n",
              "      <th>service</th>\n",
              "      <th>state</th>\n",
              "      <th>spkts</th>\n",
              "      <th>dpkts</th>\n",
              "      <th>sbytes</th>\n",
              "      <th>dbytes</th>\n",
              "      <th>rate</th>\n",
              "      <th>...</th>\n",
              "      <th>ct_dst_sport_ltm</th>\n",
              "      <th>ct_dst_src_ltm</th>\n",
              "      <th>is_ftp_login</th>\n",
              "      <th>ct_ftp_cmd</th>\n",
              "      <th>ct_flw_http_mthd</th>\n",
              "      <th>ct_src_ltm</th>\n",
              "      <th>ct_srv_dst</th>\n",
              "      <th>is_sm_ips_ports</th>\n",
              "      <th>attack_cat</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>udp</td>\n",
              "      <td>-</td>\n",
              "      <td>INT</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>496</td>\n",
              "      <td>0</td>\n",
              "      <td>90909.0902</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>Normal</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0.000008</td>\n",
              "      <td>udp</td>\n",
              "      <td>-</td>\n",
              "      <td>INT</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1762</td>\n",
              "      <td>0</td>\n",
              "      <td>125000.0003</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>Normal</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>udp</td>\n",
              "      <td>-</td>\n",
              "      <td>INT</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1068</td>\n",
              "      <td>0</td>\n",
              "      <td>200000.0051</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>Normal</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>0.000006</td>\n",
              "      <td>udp</td>\n",
              "      <td>-</td>\n",
              "      <td>INT</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>900</td>\n",
              "      <td>0</td>\n",
              "      <td>166666.6608</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>Normal</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>udp</td>\n",
              "      <td>-</td>\n",
              "      <td>INT</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2126</td>\n",
              "      <td>0</td>\n",
              "      <td>100000.0025</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>Normal</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 45 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-338e92cd-5a22-447d-8dc3-62e2d78c16da')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-a7cae8dd-b189-42a4-8cd2-a68e2be7fc9e\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a7cae8dd-b189-42a4-8cd2-a68e2be7fc9e')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-a7cae8dd-b189-42a4-8cd2-a68e2be7fc9e button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-338e92cd-5a22-447d-8dc3-62e2d78c16da button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-338e92cd-5a22-447d-8dc3-62e2d78c16da');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "# Look at the first 5 rows\n",
        "initial_data.head(n=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBfkwg51Pb88",
        "outputId": "388af14b-051a-4619-f7fc-4fb6575d22a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 82332 entries, 0 to 82331\n",
            "Data columns (total 45 columns):\n",
            " #   Column             Non-Null Count  Dtype  \n",
            "---  ------             --------------  -----  \n",
            " 0   id                 82332 non-null  int64  \n",
            " 1   dur                82332 non-null  float64\n",
            " 2   proto              82332 non-null  object \n",
            " 3   service            82332 non-null  object \n",
            " 4   state              82332 non-null  object \n",
            " 5   spkts              82332 non-null  int64  \n",
            " 6   dpkts              82332 non-null  int64  \n",
            " 7   sbytes             82332 non-null  int64  \n",
            " 8   dbytes             82332 non-null  int64  \n",
            " 9   rate               82332 non-null  float64\n",
            " 10  sttl               82332 non-null  int64  \n",
            " 11  dttl               82332 non-null  int64  \n",
            " 12  sload              82332 non-null  float64\n",
            " 13  dload              82332 non-null  float64\n",
            " 14  sloss              82332 non-null  int64  \n",
            " 15  dloss              82332 non-null  int64  \n",
            " 16  sinpkt             82332 non-null  float64\n",
            " 17  dinpkt             82332 non-null  float64\n",
            " 18  sjit               82332 non-null  float64\n",
            " 19  djit               82332 non-null  float64\n",
            " 20  swin               82332 non-null  int64  \n",
            " 21  stcpb              82332 non-null  int64  \n",
            " 22  dtcpb              82332 non-null  int64  \n",
            " 23  dwin               82332 non-null  int64  \n",
            " 24  tcprtt             82332 non-null  float64\n",
            " 25  synack             82332 non-null  float64\n",
            " 26  ackdat             82332 non-null  float64\n",
            " 27  smean              82332 non-null  int64  \n",
            " 28  dmean              82332 non-null  int64  \n",
            " 29  trans_depth        82332 non-null  int64  \n",
            " 30  response_body_len  82332 non-null  int64  \n",
            " 31  ct_srv_src         82332 non-null  int64  \n",
            " 32  ct_state_ttl       82332 non-null  int64  \n",
            " 33  ct_dst_ltm         82332 non-null  int64  \n",
            " 34  ct_src_dport_ltm   82332 non-null  int64  \n",
            " 35  ct_dst_sport_ltm   82332 non-null  int64  \n",
            " 36  ct_dst_src_ltm     82332 non-null  int64  \n",
            " 37  is_ftp_login       82332 non-null  int64  \n",
            " 38  ct_ftp_cmd         82332 non-null  int64  \n",
            " 39  ct_flw_http_mthd   82332 non-null  int64  \n",
            " 40  ct_src_ltm         82332 non-null  int64  \n",
            " 41  ct_srv_dst         82332 non-null  int64  \n",
            " 42  is_sm_ips_ports    82332 non-null  int64  \n",
            " 43  attack_cat         82332 non-null  object \n",
            " 44  label              82332 non-null  int64  \n",
            "dtypes: float64(11), int64(30), object(4)\n",
            "memory usage: 28.3+ MB\n"
          ]
        }
      ],
      "source": [
        "# information of the data: 583 data points, 10 features' columns and 1 target column\n",
        "initial_data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fGM-YfCPb88"
      },
      "source": [
        "## Tidy Data\n",
        "\n",
        "### Check missing values\n",
        "First, we should check if there are missing values in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zSGEYuUPb89",
        "outputId": "68c373f0-7306-4804-c492-895d90131b5b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id                   0\n",
              "dur                  0\n",
              "proto                0\n",
              "service              0\n",
              "state                0\n",
              "spkts                0\n",
              "dpkts                0\n",
              "sbytes               0\n",
              "dbytes               0\n",
              "rate                 0\n",
              "sttl                 0\n",
              "dttl                 0\n",
              "sload                0\n",
              "dload                0\n",
              "sloss                0\n",
              "dloss                0\n",
              "sinpkt               0\n",
              "dinpkt               0\n",
              "sjit                 0\n",
              "djit                 0\n",
              "swin                 0\n",
              "stcpb                0\n",
              "dtcpb                0\n",
              "dwin                 0\n",
              "tcprtt               0\n",
              "synack               0\n",
              "ackdat               0\n",
              "smean                0\n",
              "dmean                0\n",
              "trans_depth          0\n",
              "response_body_len    0\n",
              "ct_srv_src           0\n",
              "ct_state_ttl         0\n",
              "ct_dst_ltm           0\n",
              "ct_src_dport_ltm     0\n",
              "ct_dst_sport_ltm     0\n",
              "ct_dst_src_ltm       0\n",
              "is_ftp_login         0\n",
              "ct_ftp_cmd           0\n",
              "ct_flw_http_mthd     0\n",
              "ct_src_ltm           0\n",
              "ct_srv_dst           0\n",
              "is_sm_ips_ports      0\n",
              "attack_cat           0\n",
              "label                0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# check if there are Null values\n",
        "initial_data.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Os4FP87IPb89"
      },
      "source": [
        "A basic strategy to use incomplete datasets is to discard entire rows and/or columns containing missing values. Actually, there exists some strategies to impute missing values (see [here](https://scikit-learn.org/stable/modules/impute.html)). For simplicity, we will discard the four rows with missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3clqAAbsPb89",
        "outputId": "f3069566-cd7b-4051-e787-5733a6f43300"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(82332, 45)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "# Discard the rows with missing values\n",
        "data_to_use = initial_data.dropna()\n",
        "\n",
        "# Shape of the data: we could see that the number of rows remains the same as no null values were reported\n",
        "data_to_use.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWeAFR0tPb89"
      },
      "source": [
        "### Check imbalanced issue on y\n",
        "\n",
        "First, we get the `X` and `y1` and `y2` ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMyUcciEPb89"
      },
      "outputs": [],
      "source": [
        "X = data_to_use.drop(axis=1, columns=['attack_cat']) # X is a dataframe\n",
        "X = X.drop(axis=1, columns=['label'])\n",
        "\n",
        "y1 = data_to_use['attack_cat'].values # y is an array\n",
        "y2 = data_to_use['label'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iInxzxGYPb8-"
      },
      "outputs": [],
      "source": [
        "# Calculate Y2 ratio\n",
        "def data_ratio(y2):\n",
        "    '''\n",
        "    Calculate Y2's ratio\n",
        "    '''\n",
        "    unique, count = np.unique(y2, return_counts=True)\n",
        "    ratio = round(count[0]/count[1], 1)\n",
        "    return f'{ratio}:1 ({count[0]}/{count[1]})'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "F1jOn4niPb8-",
        "outputId": "82cc1a61-a695-44c0-ba7f-405a1090011f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-4d3989658db1>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcountplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Sum\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The class ratio for the original data:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_ratio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/seaborn/categorical.py\u001b[0m in \u001b[0;36mcountplot\u001b[0;34m(data, x, y, hue, order, hue_order, orient, color, palette, saturation, width, dodge, ax, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot pass values for both `x` and `y`\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2942\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2943\u001b[0;31m     plotter = _CountPlotter(\n\u001b[0m\u001b[1;32m   2944\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhue_order\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrorbar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_boot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/seaborn/categorical.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, hue, data, order, hue_order, estimator, errorbar, n_boot, units, seed, orient, color, palette, saturation, width, errcolor, errwidth, capsize, dodge)\u001b[0m\n\u001b[1;32m   1528\u001b[0m                  errcolor, errwidth, capsize, dodge):\n\u001b[1;32m   1529\u001b[0m         \u001b[0;34m\"\"\"Initialize the plotter.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1530\u001b[0;31m         self.establish_variables(x, y, hue, data, orient,\n\u001b[0m\u001b[1;32m   1531\u001b[0m                                  order, hue_order, units)\n\u001b[1;32m   1532\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestablish_colors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpalette\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaturation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/seaborn/categorical.py\u001b[0m in \u001b[0;36mestablish_variables\u001b[0;34m(self, x, y, hue, data, orient, order, hue_order, units)\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m                 \u001b[0;31m# Convert to a list of arrays, the common representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m                 \u001b[0mplot_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mplot_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m                 \u001b[0;31m# The group names will just be numeric indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/seaborn/categorical.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m                 \u001b[0;31m# Convert to a list of arrays, the common representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m                 \u001b[0mplot_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mplot_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m                 \u001b[0;31m# The group names will just be numeric indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'Normal'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1300x500 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.figure(figsize=(13,5))\n",
        "sns.countplot(y1,label=\"Sum\")\n",
        "plt.show()\n",
        "\n",
        "print('The class ratio for the original data:', data_ratio(y2))\n",
        "sns.countplot(y2,label=\"Sum\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qo_SOL5APb8-"
      },
      "source": [
        "We could see that the dataset is not perfectly balanced. There are some sampling techniques to deal with this issue. Here, we ignore this issue because we are aimed to implement several ML models to compare their performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FDm4nZLPb8-"
      },
      "source": [
        "### Split training and testing data\n",
        "\n",
        "It is important to split `X` and `y` as training set and testing set. Here, we will split the original data as 70% training set and 30% testing set. But the partition action from this dataset was pre configured as a training set and testing set, namely, UNSW_NB15_training-set.csv and UNSW_NB15_testing-set.csv respectively.\n",
        "\n",
        "The number of records in the training set is 175,341 records and the testing set is 82,332 records from the different types, attack and normal.Figure 1 and 2 show the testbed configuration dataset and the method of the feature creation of the UNSW-NB15, respectively.\n",
        "\n",
        "Thus the follwing code will not be utilized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRx9S8GZPb8-"
      },
      "outputs": [],
      "source": [
        "#X_train, X_test, y1_train, y1_test = train_test_split(X, y1, test_size=0.3, random_state=1)\n",
        "#print('The class ratio in training data: ', data_ratio(y1_train))\n",
        "#print('The class ratio in testing data: ', data_ratio(y1_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhM9SxUAPb8_"
      },
      "source": [
        "We will convert the orginal training data to the datframes called X_train, y1_train, y2_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCMbMCk6Pb8_"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/The UNSW-NB15 Dataset/ClassificationData/UNSW_NB15_testing-set.csv')\n",
        "X_test = test_data.drop(axis=1, columns=['attack_cat']) # X_test is a dataframe\n",
        "X_test = X_test.drop(axis=1, columns=['label'])\n",
        "\n",
        "y1_test = test_data['attack_cat'].values # y is an array\n",
        "y2_test = test_data['label'].values\n",
        "X_train = X\n",
        "y1_train = y1\n",
        "y2_train = y2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjxU8UI_Pb8_"
      },
      "source": [
        "### Transform training and testing data\n",
        "\n",
        "#### Transformation on X_train, X_test\n",
        "\n",
        "Scikit-learn provides a library of transformers. Like other estimators, these are represented by classes with a `fit` method, which learns model parameters (e.g. mean and standard deviation for normalization) from a training set, and a `transform` method which applies this transformation model to unseen data.\n",
        "\n",
        "**NOTE: The reason of performing transformation after splitting the original data is that we will `fit` those parameters on training set**.\n",
        "\n",
        "In addition, it is very common to want to perform different data transformation techniques on different columns in your input data. The `ColumnTransformer` is a class in the scikit-learn library that allows you to selectively apply data preparation transforms. For example, it allows you to apply a specific transform or sequence of transforms to just the numerical columns, and a separate sequence of transforms to just the categorical columns.\n",
        "\n",
        "In our case, we need to perform `OneHotEncoder` on `Gender` column because it is categorical, and perform `StandardScaler` on other numerical columns.\n",
        "\n",
        "- `OneHotEncoder`: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\n",
        "- `StandardScaler`: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
        "\n",
        "First, we find out which columns are categorical and which are numerical."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_Nq-l8mPb8_"
      },
      "outputs": [],
      "source": [
        "# determine categorical and numerical columns\n",
        "numerical_cols = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_cols = X_train.select_dtypes(include=['object', 'bool']).columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsFD7jSxPb8_",
        "outputId": "b3df70db-e880-45c1-b7b6-15c76742aff0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['id', 'dur', 'spkts', 'dpkts', 'sbytes', 'dbytes', 'rate', 'sttl',\n",
              "       'dttl', 'sload', 'dload', 'sloss', 'dloss', 'sinpkt', 'dinpkt', 'sjit',\n",
              "       'djit', 'swin', 'stcpb', 'dtcpb', 'dwin', 'tcprtt', 'synack', 'ackdat',\n",
              "       'smean', 'dmean', 'trans_depth', 'response_body_len', 'ct_srv_src',\n",
              "       'ct_state_ttl', 'ct_dst_ltm', 'ct_src_dport_ltm', 'ct_dst_sport_ltm',\n",
              "       'ct_dst_src_ltm', 'is_ftp_login', 'ct_ftp_cmd', 'ct_flw_http_mthd',\n",
              "       'ct_src_ltm', 'ct_srv_dst', 'is_sm_ips_ports'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "numerical_cols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thVr8CYGPb9A",
        "outputId": "ef36600d-8628-4331-9630-d7d329803848"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['proto', 'service', 'state'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "categorical_cols"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vR6zb0A0Pb9A"
      },
      "source": [
        "Then, we construct the `ColumnTransformer` object, and then fit it on training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "3s7ia7RaPb9A",
        "outputId": "1821defa-58e5-4f9d-91da-53cb0d8c7095"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ColumnTransformer(transformers=[('ohe', OneHotEncoder(drop='first'),\n",
              "                                 Index(['proto', 'service', 'state'], dtype='object')),\n",
              "                                ('scale', StandardScaler(),\n",
              "                                 Index(['id', 'dur', 'spkts', 'dpkts', 'sbytes', 'dbytes', 'rate', 'sttl',\n",
              "       'dttl', 'sload', 'dload', 'sloss', 'dloss', 'sinpkt', 'dinpkt', 'sjit',\n",
              "       'djit', 'swin', 'stcpb', 'dtcpb', 'dwin', 'tcprtt', 'synack', 'ackdat',\n",
              "       'smean', 'dmean', 'trans_depth', 'response_body_len', 'ct_srv_src',\n",
              "       'ct_state_ttl', 'ct_dst_ltm', 'ct_src_dport_ltm', 'ct_dst_sport_ltm',\n",
              "       'ct_dst_src_ltm', 'is_ftp_login', 'ct_ftp_cmd', 'ct_flw_http_mthd',\n",
              "       'ct_src_ltm', 'ct_srv_dst', 'is_sm_ips_ports'],\n",
              "      dtype='object'))])"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>ColumnTransformer(transformers=[(&#x27;ohe&#x27;, OneHotEncoder(drop=&#x27;first&#x27;),\n",
              "                                 Index([&#x27;proto&#x27;, &#x27;service&#x27;, &#x27;state&#x27;], dtype=&#x27;object&#x27;)),\n",
              "                                (&#x27;scale&#x27;, StandardScaler(),\n",
              "                                 Index([&#x27;id&#x27;, &#x27;dur&#x27;, &#x27;spkts&#x27;, &#x27;dpkts&#x27;, &#x27;sbytes&#x27;, &#x27;dbytes&#x27;, &#x27;rate&#x27;, &#x27;sttl&#x27;,\n",
              "       &#x27;dttl&#x27;, &#x27;sload&#x27;, &#x27;dload&#x27;, &#x27;sloss&#x27;, &#x27;dloss&#x27;, &#x27;sinpkt&#x27;, &#x27;dinpkt&#x27;, &#x27;sjit&#x27;,\n",
              "       &#x27;djit&#x27;, &#x27;swin&#x27;, &#x27;stcpb&#x27;, &#x27;dtcpb&#x27;, &#x27;dwin&#x27;, &#x27;tcprtt&#x27;, &#x27;synack&#x27;, &#x27;ackdat&#x27;,\n",
              "       &#x27;smean&#x27;, &#x27;dmean&#x27;, &#x27;trans_depth&#x27;, &#x27;response_body_len&#x27;, &#x27;ct_srv_src&#x27;,\n",
              "       &#x27;ct_state_ttl&#x27;, &#x27;ct_dst_ltm&#x27;, &#x27;ct_src_dport_ltm&#x27;, &#x27;ct_dst_sport_ltm&#x27;,\n",
              "       &#x27;ct_dst_src_ltm&#x27;, &#x27;is_ftp_login&#x27;, &#x27;ct_ftp_cmd&#x27;, &#x27;ct_flw_http_mthd&#x27;,\n",
              "       &#x27;ct_src_ltm&#x27;, &#x27;ct_srv_dst&#x27;, &#x27;is_sm_ips_ports&#x27;],\n",
              "      dtype=&#x27;object&#x27;))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;ohe&#x27;, OneHotEncoder(drop=&#x27;first&#x27;),\n",
              "                                 Index([&#x27;proto&#x27;, &#x27;service&#x27;, &#x27;state&#x27;], dtype=&#x27;object&#x27;)),\n",
              "                                (&#x27;scale&#x27;, StandardScaler(),\n",
              "                                 Index([&#x27;id&#x27;, &#x27;dur&#x27;, &#x27;spkts&#x27;, &#x27;dpkts&#x27;, &#x27;sbytes&#x27;, &#x27;dbytes&#x27;, &#x27;rate&#x27;, &#x27;sttl&#x27;,\n",
              "       &#x27;dttl&#x27;, &#x27;sload&#x27;, &#x27;dload&#x27;, &#x27;sloss&#x27;, &#x27;dloss&#x27;, &#x27;sinpkt&#x27;, &#x27;dinpkt&#x27;, &#x27;sjit&#x27;,\n",
              "       &#x27;djit&#x27;, &#x27;swin&#x27;, &#x27;stcpb&#x27;, &#x27;dtcpb&#x27;, &#x27;dwin&#x27;, &#x27;tcprtt&#x27;, &#x27;synack&#x27;, &#x27;ackdat&#x27;,\n",
              "       &#x27;smean&#x27;, &#x27;dmean&#x27;, &#x27;trans_depth&#x27;, &#x27;response_body_len&#x27;, &#x27;ct_srv_src&#x27;,\n",
              "       &#x27;ct_state_ttl&#x27;, &#x27;ct_dst_ltm&#x27;, &#x27;ct_src_dport_ltm&#x27;, &#x27;ct_dst_sport_ltm&#x27;,\n",
              "       &#x27;ct_dst_src_ltm&#x27;, &#x27;is_ftp_login&#x27;, &#x27;ct_ftp_cmd&#x27;, &#x27;ct_flw_http_mthd&#x27;,\n",
              "       &#x27;ct_src_ltm&#x27;, &#x27;ct_srv_dst&#x27;, &#x27;is_sm_ips_ports&#x27;],\n",
              "      dtype=&#x27;object&#x27;))])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ohe</label><div class=\"sk-toggleable__content\"><pre>Index([&#x27;proto&#x27;, &#x27;service&#x27;, &#x27;state&#x27;], dtype=&#x27;object&#x27;)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(drop=&#x27;first&#x27;)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">scale</label><div class=\"sk-toggleable__content\"><pre>Index([&#x27;id&#x27;, &#x27;dur&#x27;, &#x27;spkts&#x27;, &#x27;dpkts&#x27;, &#x27;sbytes&#x27;, &#x27;dbytes&#x27;, &#x27;rate&#x27;, &#x27;sttl&#x27;,\n",
              "       &#x27;dttl&#x27;, &#x27;sload&#x27;, &#x27;dload&#x27;, &#x27;sloss&#x27;, &#x27;dloss&#x27;, &#x27;sinpkt&#x27;, &#x27;dinpkt&#x27;, &#x27;sjit&#x27;,\n",
              "       &#x27;djit&#x27;, &#x27;swin&#x27;, &#x27;stcpb&#x27;, &#x27;dtcpb&#x27;, &#x27;dwin&#x27;, &#x27;tcprtt&#x27;, &#x27;synack&#x27;, &#x27;ackdat&#x27;,\n",
              "       &#x27;smean&#x27;, &#x27;dmean&#x27;, &#x27;trans_depth&#x27;, &#x27;response_body_len&#x27;, &#x27;ct_srv_src&#x27;,\n",
              "       &#x27;ct_state_ttl&#x27;, &#x27;ct_dst_ltm&#x27;, &#x27;ct_src_dport_ltm&#x27;, &#x27;ct_dst_sport_ltm&#x27;,\n",
              "       &#x27;ct_dst_src_ltm&#x27;, &#x27;is_ftp_login&#x27;, &#x27;ct_ftp_cmd&#x27;, &#x27;ct_flw_http_mthd&#x27;,\n",
              "       &#x27;ct_src_ltm&#x27;, &#x27;ct_srv_dst&#x27;, &#x27;is_sm_ips_ports&#x27;],\n",
              "      dtype=&#x27;object&#x27;)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "# define the transformation methods for the columns\n",
        "t = [('ohe', OneHotEncoder(drop='first'), categorical_cols),\n",
        "    ('scale', StandardScaler(), numerical_cols)]\n",
        "\n",
        "col_trans = ColumnTransformer(transformers=t)\n",
        "\n",
        "# fit the transformation on training data\n",
        "col_trans.fit(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPfpZJF2Pb9A"
      },
      "outputs": [],
      "source": [
        "X_train_transform = col_trans.transform(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvCTB2-BPb9A"
      },
      "outputs": [],
      "source": [
        "# apply transformation to both training and testing data\n",
        "# fit the transformation on training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "-YYBb4OVPb9B",
        "outputId": "dbaeab4e-139c-4c02-d43f-2dda50c7c386"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-c208a8665290>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_test_transform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcol_trans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/compose/_column_transformer.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    798\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_n_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 800\u001b[0;31m         Xs = self._fit_transform(\n\u001b[0m\u001b[1;32m    801\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/compose/_column_transformer.py\u001b[0m in \u001b[0;36m_fit_transform\u001b[0;34m(self, X, y, func, fitted, column_as_strings)\u001b[0m\n\u001b[1;32m    656\u001b[0m         )\n\u001b[1;32m    657\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 658\u001b[0;31m             return Parallel(n_jobs=self.n_jobs)(\n\u001b[0m\u001b[1;32m    659\u001b[0m                 delayed(func)(\n\u001b[1;32m    660\u001b[0m                     \u001b[0mtransformer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrans\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfitted\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtrans\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1853\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1854\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1855\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1856\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1857\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1784\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_transform_one\u001b[0;34m(transformer, X, y, weight, **fit_params)\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_transform_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 876\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    877\u001b[0m     \u001b[0;31m# if we have a weight for this transformer, multiply output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    915\u001b[0m             \u001b[0;34m\"infrequent_if_exist\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m         }\n\u001b[0;32m--> 917\u001b[0;31m         X_int, X_mask = self._transform(\n\u001b[0m\u001b[1;32m    918\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0mhandle_unknown\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_unknown\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, X, handle_unknown, force_all_finite, warn_on_unknown)\u001b[0m\n\u001b[1;32m    172\u001b[0m                         \u001b[0;34m\" during transform\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m                     )\n\u001b[0;32m--> 174\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mwarn_on_unknown\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found unknown categories ['icmp', 'rtp'] in column 0 during transform"
          ]
        }
      ],
      "source": [
        "X_test_transform = col_trans.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TCnBlLAPb9B"
      },
      "source": [
        "We could look at the transformed training data. It becomes an array-like structure rather than a dataframe structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7A7rHWfPb9B",
        "outputId": "d022c055-0c64-45c2-b64a-cf46bea90d77"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(82332, 188)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "# look at the transformed training data\n",
        "X_train_transform.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "dzWC9eJyPb9B",
        "outputId": "53ef9980-bc44-4db1-9d32-faec4aabee2d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-39f1a0ed9830>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_test_transform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'X_test_transform' is not defined"
          ]
        }
      ],
      "source": [
        "X_test_transform.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVvvussdPb9B"
      },
      "source": [
        "#### Transformation on y_train and y_test\n",
        "\n",
        "`LabelEncoder` is a utility class to help normalize labels such that they contain only values between 0 and n_classes-1.  Although it should be much easier to achieve this by subtracting 1 from the `y` array, we provide the `LabelEncoder` transformation which is a standard way of dealing with targeted values. Again, similar to the transformation on `X`, we will apply `fit` method to `y_train` and then apply `transform` method to both `y_train` and `y_test`.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IAPaNlsxPb9C"
      },
      "outputs": [],
      "source": [
        "# Note that the distinct values/labels in `y2` target are 1 and 2.\n",
        "pd.unique(y1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQc193qPPb9C"
      },
      "outputs": [],
      "source": [
        "# Define a LabelEncoder() transformation method and fit on y1_train\n",
        "target_trans = LabelEncoder()\n",
        "target_trans.fit(y1_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGo7XHVvPb9C"
      },
      "outputs": [],
      "source": [
        "# apply transformation method on y1_train and y1_test\n",
        "y1_train_transform = target_trans.transform(y1_train)\n",
        "y1_test_transform = target_trans.transform(y1_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bR7kE4V0Pb9C"
      },
      "outputs": [],
      "source": [
        "# view the transformed y1_train\n",
        "y1_train_transform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeTFxqMxPb9C"
      },
      "source": [
        "# Train ML Models\n",
        "\n",
        "We will train several machine learning models for the training set and evaluate their performance on both training and testing set. Before doing this, let's first go through a standard procedure of training a certain classifier.\n",
        "\n",
        "## Steps of Training Model\n",
        "\n",
        "- Step 1: Train a ML model and validate it via 5-fold cross-validation (CV). The CV results will show how good the model has been trained by using the training data given a set of hyperparameters in the ML model. The metrics of evaluating a model include accuracy, precision, recall, F1 score, AUC value of ROC.\n",
        "\n",
        "\n",
        "- Step 2: Evaluate the model by using the testing data. It will show how good the model could be used to make predictions for unseen data.\n",
        "\n",
        "**NOTE: For simplicity, we do not tune hyperparameters in the ML model and will use the default settings of hyperparameters in each ML model.**\n",
        "\n",
        "**Let's firstly train a `Logistic Regression` model with regards to Step 1 and Step 2.** using `y2` as the response feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RePA28SBPb9D"
      },
      "outputs": [],
      "source": [
        "# ===== Step 1: cross-validation ========\n",
        "# define a Logistic Regression classifier\n",
        "clf = LogisticRegression(solver='lbfgs', random_state=123, max_iter = 4000, multi_class = \"ovr\")\n",
        "\n",
        "# define  Stratified 5-fold cross-validator, it provides train/validate indices to split data in train/validate sets.\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n",
        "\n",
        "# define metrics for evaluating\n",
        "scoring = ['accuracy', 'precision_micro', 'recall_micro', 'f1_micro', 'roc_auc_ovr']\n",
        "\n",
        "# perform the 5-fold CV and get the metrics results\n",
        "cv_results = cross_validate(estimator=clf,\n",
        "                            X=X_train_transform,\n",
        "                            y=y1_train_transform,\n",
        "                            scoring=scoring,\n",
        "                            cv=cv,\n",
        "                            return_train_score=False) # prevent to show the train scores on cv splits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ooiie4EdPb9E"
      },
      "outputs": [],
      "source": [
        "cv_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_xfXmbbPb9E"
      },
      "source": [
        "The CV results include:\n",
        "\n",
        "- `test_score`: The score array for test scores on each cv split. Suffix `_score` in `test_score` changes to a specific metric like `test_accuracy` or `test_f1` if there are multiple scoring metrics in the scoring parameter.\n",
        "\n",
        "- `fit_time`: The time for fitting the estimator on the train set for each cv split.\n",
        "\n",
        "- `score_time`: The time for scoring the estimator on the test set for each cv split.\n",
        "\n",
        "**Typically, we will use the mean value of each metric to represent the evaluation results of cross-validation.** For example, we could calculate the mean value of the `accuracy` score:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXZwbuajPb9E"
      },
      "outputs": [],
      "source": [
        "cv_results['test_accuracy'].mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsG1wkUdPb9E"
      },
      "source": [
        "**In addition, the cross-validation step is used to find the best set of hyperparameters which give the \"best\" scores of metrics.** Since we do not tune hyperparameters in this case, we then directly fit the Logistic Regression model by using the default values of hyperparameters and evaluate it on testing data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dh8yGHEuPb9F"
      },
      "outputs": [],
      "source": [
        "# ======== Step 2: Evaluate the model using testing data =======\n",
        "\n",
        "# fit the Logistic Regression model\n",
        "clf.fit(X=X_train_transform, y=y1_train_transform)\n",
        "\n",
        "# predition on testing data\n",
        "y_pred_class = clf.predict(X=X_test_transform)\n",
        "y_pred_score = clf.predict_proba(X=X_test_transform)[:, 1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sT-ZRquPb9F"
      },
      "outputs": [],
      "source": [
        "# ======== Step 2: Evaluate the model using testing data =======\n",
        "\n",
        "# fit the Logistic Regression model\n",
        "clf.fit(X=X_train_transform, y=y1_train_transform)\n",
        "\n",
        "# predition on testing data\n",
        "y_pred_class = clf.predict(X=X_test_transform)\n",
        "y_pred_score = clf.predict_proba(X=X_test_transform)[:, 1]\n",
        "\n",
        "# AUC of ROC\n",
        "#auc_ontest = roc_auc_score(y_true=y1_test_transform, y_score=y_pred_score, average='micro', multi_class='ovr', max_fpr = 'None' )\n",
        "# confusion matrix\n",
        "cm_ontest = confusion_matrix(y_true=y1_test_transform, y_pred=y_pred_class)\n",
        "# precision score\n",
        "precision_ontest = precision_score(y_true=y1_test_transform, y_pred=y_pred_class, average='micro')\n",
        "# recall score\n",
        "recall_ontest = recall_score(y_true=y1_test_transform, y_pred=y_pred_class, average='micro')\n",
        "# classifition report\n",
        "cls_report_ontest = classification_report(y_true=y1_test_transform, y_pred=y_pred_class)\n",
        "\n",
        "# print the above results\n",
        "#print('The model scores {:1.5f} ROC AUC on the test set.'.format(auc_ontest))\n",
        "print('The precision score on the test set: {:1.5f}'.format(precision_ontest))\n",
        "print('The recall score on the test set: {:1.5f}'.format(recall_ontest))\n",
        "print('Confusion Matrix:\\n', cm_ontest)\n",
        "# Print classification report:\n",
        "print('Classification Report:\\n', cls_report_ontest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ro0WtU07Pb9F"
      },
      "outputs": [],
      "source": [
        "# AUC of ROC\n",
        "auc_ontest = roc_auc_score(y_true=y1_test_transform, y_score=y_pred_score, average='micro', multi_class='ovr', max_fpr = 'None' )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pfv3uspEPb9F"
      },
      "source": [
        "Through the above steps, we could assess if the trained model is good for making predictions on unseen data. Recall that we are training a ML model to classify types of attack behavior on network packets. Considering that we prefer to having a model to capture the cases of attack as many as possible. In other words, the favorable model could have relatively high \"coverage\" ability and high \"precision\" ability. **Therefore, we could choose `F1 score` as the evaluation metric in this case. `F1 score` can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHuLcyr-Pb9G"
      },
      "source": [
        "## Several ML Models\n",
        "\n",
        "We will implement several ML models through the above steps. The only difference part is to change `clf = LogisticRegression()` as other model functions, for example, `clf = DecisionTreeClassifier()`.\n",
        "\n",
        "The followings are ML models functions:\n",
        "\n",
        "- `LogisticRegression()`\n",
        "- `DecisionTreeClassifier()`\n",
        "- `RandomForestClassifier()`\n",
        "- `MLPClassifier()`\n",
        "\n",
        "Note that in `MLPClassifier()` we set the solver as `adam` which has better performance for big data. Also, we set the maximum iterations as 8000 to ensure convergence. `random_state` is used to ensure reproducible results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l96t1lmCPb9G"
      },
      "outputs": [],
      "source": [
        "# Define four models\n",
        "models = [('LogisticRegression', LogisticRegression(random_state=123, max_iter=5000, multi_class = \"ovr\")),\n",
        "          ('DecisionTree', DecisionTreeClassifier(random_state=123)),\n",
        "          ('RandomForest', RandomForestClassifier(random_state=123)),\n",
        "          #('MultiLayerPerceptron', MLPClassifier(random_state=123, solver='adam', max_iter=8000))\n",
        "         ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00XQB9gLPb9G"
      },
      "source": [
        "We could check the hyperparameters values in these models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9AC31viPb9G"
      },
      "outputs": [],
      "source": [
        "for model_name, clf in models:\n",
        "    print(clf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfOd6Pg1Pb9G"
      },
      "source": [
        "**Finally, we write the code to perform the above four ML models and store their cross-validation results and evaluation results on testing data.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3GLDOWzmPb9G"
      },
      "outputs": [],
      "source": [
        "# define several lists and dataframe to store the CV results and evaluation results on testing data\n",
        "model_names_list = []\n",
        "cv_fit_time_mean_list = []\n",
        "cv_accuracy_mean_list = []\n",
        "cv_precision_mean_list = []\n",
        "cv_recall_mean_list = []\n",
        "cv_f1_mean_list = []\n",
        "#cv_roc_auc_mean_list = []\n",
        "\n",
        "test_accuracy_list = []\n",
        "test_precision_list = []\n",
        "test_recall_list = []\n",
        "test_f1_list = []\n",
        "#test_roc_auc_list = []\n",
        "\n",
        "#test_roc_curve_df = pd.DataFrame()\n",
        "\n",
        "\n",
        "for model_name, clf in models:\n",
        "\n",
        "    # ==== Step 1: Cross-validation =====\n",
        "\n",
        "    # define  Stratified 5-fold cross-validator\n",
        "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n",
        "    # define metrics for evaluating\n",
        "    scoring = ['accuracy', 'precision_micro', 'recall_micro', 'f1_micro']\n",
        "    # perform the 5-fold CV and get the metrics results\n",
        "    cv_results = cross_validate(estimator=clf,\n",
        "                                X=X_train_transform,\n",
        "                                y=y1_train_transform,\n",
        "                                scoring=scoring,\n",
        "                                cv=cv,\n",
        "                                return_train_score=False)  # prevent to show the train scores on cv splits.\n",
        "\n",
        "    # calculate the mean values of those scores\n",
        "    cv_fit_time_mean = cv_results['fit_time'].mean()\n",
        "    cv_accuracy_mean = cv_results['test_accuracy'].mean()\n",
        "    cv_precision_mean = cv_results['test_precision_micro'].mean()\n",
        "    cv_recall_mean = cv_results['test_recall_micro'].mean()\n",
        "    cv_f1_mean = cv_results['test_f1_micro'].mean()\n",
        "    #cv_roc_auc_mean = cv_results['test_roc_auc_ovr'].mean()\n",
        "\n",
        "    # store CV results into those lists\n",
        "    model_names_list.append(model_name)\n",
        "    cv_fit_time_mean_list.append(cv_fit_time_mean)\n",
        "    cv_accuracy_mean_list.append(cv_accuracy_mean)\n",
        "    cv_precision_mean_list.append(cv_precision_mean)\n",
        "    cv_recall_mean_list.append(cv_recall_mean)\n",
        "    cv_f1_mean_list.append(cv_f1_mean)\n",
        "    #cv_roc_auc_mean_list.append(cv_roc_auc_mean)\n",
        "\n",
        "    # ==== Step 2: Evaluation on Testing data =====\n",
        "\n",
        "    # fit model\n",
        "    clf.fit(X=X_train_transform, y=y1_train_transform)\n",
        "\n",
        "    # predition on testing data\n",
        "\n",
        "    # predicted label or class\n",
        "    y_pred_class = clf.predict(X=X_test_transform)\n",
        "\n",
        "    # predicted probability of the label 1\n",
        "    y_pred_score = clf.predict_proba(X=X_test_transform)[:, 1]\n",
        "\n",
        "    # accuracy\n",
        "    accuracy_ontest = accuracy_score(y_true=y1_test_transform, y_pred=y_pred_class)\n",
        "\n",
        "    # auc of ROC\n",
        "    #auc_ontest = roc_auc_score(y_true=y1_test_transform, y_score=y_pred_score, average = 'micro', multi_class = 'ovr')\n",
        "\n",
        "    # precision score\n",
        "    precision_ontest = precision_score(y_true=y1_test_transform, y_pred=y_pred_class, average = 'micro')\n",
        "\n",
        "    # recall score\n",
        "    recall_ontest = recall_score(y_true=y1_test_transform, y_pred=y_pred_class, average = 'micro')\n",
        "\n",
        "    # F1 score\n",
        "    f1_ontest = f1_score(y_true=y1_test_transform, y_pred=y_pred_class, average = 'micro')\n",
        "\n",
        "    # roc curve dataframe\n",
        "    #fpr, tpr, threshold_roc = roc_curve(y_true=y1_test_transform, y_score=y_pred_score)\n",
        "\n",
        "    #roc_df = pd.DataFrame(list(zip(fpr, tpr, threshold_roc)),\n",
        "                          #columns=['False Positive Rate', 'True Positive Rate', 'Threshold'])\n",
        "\n",
        "    #roc_df['Model'] = '{} (AUC = {:.3f})'.format(model_name, auc_ontest)\n",
        "\n",
        "\n",
        "    # store the above values\n",
        "    test_accuracy_list.append(accuracy_ontest)\n",
        "    #test_roc_auc_list.append(auc_ontest)\n",
        "    test_precision_list.append(precision_ontest)\n",
        "    test_recall_list.append(recall_ontest)\n",
        "    test_f1_list.append(f1_ontest)\n",
        "\n",
        "    #test_roc_curve_df = pd.concat([test_roc_curve_df, roc_df],\n",
        "                                 #ignore_index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__ZE1gRdPb9H"
      },
      "source": [
        "### Model Comparison\n",
        "\n",
        "We've stored CV results and evaluation results of testing data for the three ML models. Then, we could create a dataframe to view them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8i4RNoq2Pb9H"
      },
      "outputs": [],
      "source": [
        "results_dict1 = {'Model Name': model_names_list,\n",
        "                'CV Fit Time': cv_fit_time_mean_list,\n",
        "                'CV Accuracy mean': cv_accuracy_mean_list,\n",
        "                'CV Precision mean': cv_precision_mean_list,\n",
        "                'CV Recall mean': cv_recall_mean_list,\n",
        "                'CV F1 mean': cv_f1_mean_list,\n",
        "                #'CV AUC mean': cv_roc_auc_mean_list,\n",
        "                'Test Accuracy': test_accuracy_list,\n",
        "                'Test Precision': test_precision_list,\n",
        "                'Test Recall': test_recall_list,\n",
        "                'Test F1': test_f1_list,\n",
        "                #'Test AUC': test_roc_auc_list\n",
        "                }\n",
        "\n",
        "results1_df = pd.DataFrame(results_dict)\n",
        "\n",
        "# sort the results according to F1 score on testing data\n",
        "results_df1.sort_values(by='Test F1', ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hcRDtpcPb9H"
      },
      "source": [
        "## ML Models: MLPClassifier\n",
        "\n",
        "Note that in `MLPClassifier()` we set the solver as `adam` which has better performance for big data. Also, we set the maximum iterations as 8000 to ensure convergence. `random_state` is used to ensure reproducible results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5JoJoABPb9H"
      },
      "outputs": [],
      "source": [
        "# Define MLP Classifier\n",
        "MLPmodel = [('MultiLayerPerceptron', MLPClassifier(random_state=123, solver='adam', max_iter=8000))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0NpY9s4Pb9H"
      },
      "outputs": [],
      "source": [
        "for model_name, clf in MLPmodel:\n",
        "    print(clf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uiKdFD20Pb9H"
      },
      "outputs": [],
      "source": [
        "# define several lists and dataframe to store the CV results and evaluation results on testing data\n",
        "model_names_list = []\n",
        "cv_fit_time_mean_list = []\n",
        "cv_accuracy_mean_list = []\n",
        "cv_precision_mean_list = []\n",
        "cv_recall_mean_list = []\n",
        "cv_f1_mean_list = []\n",
        "#cv_roc_auc_mean_list = []\n",
        "\n",
        "test_accuracy_list = []\n",
        "test_precision_list = []\n",
        "test_recall_list = []\n",
        "test_f1_list = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Edyoro90Pb9H"
      },
      "outputs": [],
      "source": [
        "for model_name, clf in MLPmodel:\n",
        "\n",
        "    # ==== Step 1: Cross-validation =====\n",
        "\n",
        "    # define  Stratified 5-fold cross-validator\n",
        "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n",
        "    # define metrics for evaluating\n",
        "    scoring = ['accuracy', 'precision_micro', 'recall_micro', 'f1_micro']\n",
        "    # perform the 5-fold CV and get the metrics results\n",
        "    cv_results = cross_validate(estimator=clf,\n",
        "                                X=X_train_transform,\n",
        "                                y=y1_train_transform,\n",
        "                                scoring=scoring,\n",
        "                                cv=cv,\n",
        "                                return_train_score=False)  # prevent to show the train scores on cv splits.\n",
        "\n",
        "    # calculate the mean values of those scores\n",
        "    cv_fit_time_mean = cv_results['fit_time'].mean()\n",
        "    cv_accuracy_mean = cv_results['test_accuracy'].mean()\n",
        "    cv_precision_mean = cv_results['test_precision_micro'].mean()\n",
        "    cv_recall_mean = cv_results['test_recall_micro'].mean()\n",
        "    cv_f1_mean = cv_results['test_f1_micro'].mean()\n",
        "\n",
        "    # store CV results into those lists\n",
        "    model_names_list.append(model_name)\n",
        "    cv_fit_time_mean_list.append(cv_fit_time_mean)\n",
        "    cv_accuracy_mean_list.append(cv_accuracy_mean)\n",
        "    cv_precision_mean_list.append(cv_precision_mean)\n",
        "    cv_recall_mean_list.append(cv_recall_mean)\n",
        "    cv_f1_mean_list.append(cv_f1_mean)\n",
        "\n",
        "    # ==== Step 2: Evaluation on Testing data =====\n",
        "\n",
        "    # fit model\n",
        "    clf.fit(X=X_train_transform, y=y1_train_transform)\n",
        "\n",
        "    # predition on testing data\n",
        "\n",
        "    # predicted label or class\n",
        "    y_pred_class = clf.predict(X=X_test_transform)\n",
        "\n",
        "    # predicted probability of the label 1\n",
        "    y_pred_score = clf.predict_proba(X=X_test_transform)[:, 1]\n",
        "\n",
        "    # accuracy\n",
        "    accuracy_ontest = accuracy_score(y_true=y1_test_transform, y_pred=y_pred_class)\n",
        "\n",
        "\n",
        "    # precision score\n",
        "    precision_ontest = precision_score(y_true=y1_test_transform, y_pred=y_pred_class, average = 'micro')\n",
        "\n",
        "    # recall score\n",
        "    recall_ontest = recall_score(y_true=y1_test_transform, y_pred=y_pred_class, average = 'micro')\n",
        "\n",
        "    # F1 score\n",
        "    f1_ontest = f1_score(y_true=y1_test_transform, y_pred=y_pred_class, average = 'micro')\n",
        "\n",
        "    # store the above values\n",
        "    test_accuracy_list.append(accuracy_ontest)\n",
        "    #test_roc_auc_list.append(auc_ontest)\n",
        "    test_precision_list.append(precision_ontest)\n",
        "    test_recall_list.append(recall_ontest)\n",
        "    test_f1_list.append(f1_ontest)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PbejwJ3mPb9I"
      },
      "outputs": [],
      "source": [
        "results_dict2 = {'Model Name': model_names_list,\n",
        "                'CV Fit Time': cv_fit_time_mean_list,\n",
        "                'CV Accuracy mean': cv_accuracy_mean_list,\n",
        "                'CV Precision mean': cv_precision_mean_list,\n",
        "                'CV Recall mean': cv_recall_mean_list,\n",
        "                'CV F1 mean': cv_f1_mean_list,\n",
        "                #'CV AUC mean': cv_roc_auc_mean_list,\n",
        "                'Test Accuracy': test_accuracy_list,\n",
        "                'Test Precision': test_precision_list,\n",
        "                'Test Recall': test_recall_list,\n",
        "                'Test F1': test_f1_list,\n",
        "                #'Test AUC': test_roc_auc_list\n",
        "                }\n",
        "\n",
        "results2_df = pd.DataFrame(results_dict2)\n",
        "\n",
        "# sort the results according to F1 score on testing data\n",
        "results2_df.sort_values(by='Test F1', ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g839ohjdPb9I"
      },
      "outputs": [],
      "source": [
        "ResultsMASTER = results1_df.append(results2_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPYWe6fsPb9I"
      },
      "outputs": [],
      "source": [
        "# sort the results according to F1 score on testing data\n",
        "ResultsMASTER.sort_values(by='Test F1', ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3xp8PDMPb9I"
      },
      "source": [
        "#### F1 Score Comparison\n",
        "\n",
        "As we discussed in the section 4.1, we choose `F1 score` as the evaluation metric on testing data to show the performance of the trained model. According to the above table, we could see that `RandomForest` model outperforms others because it has the highest F1 score.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YwuIwx0KPb9I"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "332px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}